{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "#### Active learning NNGP experiment\n",
    "We use one set of params for simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model.mlp import MLP\n",
    "from dataloader.rosen import RosenData\n",
    "from uncertainty_estimator.nngp import NNGP\n",
    "from sample_selector.eager import EagerSampleSelector\n",
    "from oracle.identity import IdentityOracle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'random_state': 4623457,\n",
    "    'n_dim': 10,\n",
    "    'n_train': 200,\n",
    "    'n_test': 200,\n",
    "    'n_pool': 1000,\n",
    "    'layers': [128, 64, 32],\n",
    "    'update_sample_size': 10,\n",
    "    'al_iterations': 10\n",
    "}\n",
    "\n",
    "np.random.seed(config['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_shapes(note, *sets):\n",
    "    print(note)\n",
    "    for x, y in sets:\n",
    "        print(\"shapes:\", x.shape, y.shape)\n",
    "\n",
    "# load data\n",
    "X_train, y_train, _, _, X_test, y_test, X_pool, y_pool = RosenData(\n",
    "    config['n_train'], 0, config['n_test'], config['n_pool'], config['n_dim']\n",
    ").dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/beardysome/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /media/media_drive/skoltech/NNGP/model/mlp.py:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/beardysome/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Init neural network & tf session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = MLP(\n",
    "    ndim = config['n_dim'],\n",
    "    random_state = config['random_state'],\n",
    "    layers = config['layers']\n",
    ")\n",
    "\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "session_config = tf.ConfigProto()\n",
    "session_config.gpu_options.allow_growth = True\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session(config=session_config)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00742023, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00219058, 0.        , 0.        ,\n",
       "       0.00603731, 0.        , 0.00509445, 0.        , 0.        ,\n",
       "       0.00231341, 0.        , 0.00282036, 0.00510296, 0.00617693,\n",
       "       0.00488377, 0.00154529, 0.00309945, 0.004882  , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00540273, 0.00697696,\n",
       "       0.00264545, 0.        , 0.        , 0.00753367, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00224714, 0.00478362,\n",
       "       0.00064051, 0.        , 0.00114898, 0.        , 0.00844913,\n",
       "       0.00291686, 0.00408292, 0.        , 0.00486653, 0.00558084,\n",
       "       0.00240489, 0.        , 0.00568766, 0.        , 0.        ,\n",
       "       0.        , 0.00715222, 0.00380999, 0.        , 0.00254945,\n",
       "       0.00344842, 0.00388168, 0.00598584, 0.00758747, 0.        ,\n",
       "       0.0014556 , 0.        , 0.00849379, 0.00258676, 0.        ,\n",
       "       0.00274633, 0.        , 0.00339727, 0.        , 0.00763367,\n",
       "       0.00376858, 0.        , 0.        , 0.00249387, 0.00716513,\n",
       "       0.0068673 , 0.00609508, 0.        , 0.00050837, 0.00794548,\n",
       "       0.00300571, 0.00564901, 0.00151915, 0.        , 0.00425137,\n",
       "       0.00623149, 0.00491196, 0.        , 0.        , 0.00562617,\n",
       "       0.        , 0.00209129, 0.00834501, 0.00141858, 0.        ,\n",
       "       0.00311305, 0.00407852, 0.        , 0.        , 0.        ,\n",
       "       0.00725542, 0.        , 0.00249134, 0.00293834, 0.00656539,\n",
       "       0.        , 0.00483407, 0.00577129, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00568082, 0.00436684,\n",
       "       0.        , 0.        , 0.        , 0.00562505, 0.00837434,\n",
       "       0.00113306, 0.        , 0.00625532, 0.0061921 , 0.00237734,\n",
       "       0.00857562, 0.        , 0.        , 0.00490731, 0.        ,\n",
       "       0.00443661, 0.        , 0.00341797, 0.        , 0.00136581,\n",
       "       0.00479209, 0.00601096, 0.00448081, 0.        , 0.        ,\n",
       "       0.00580973, 0.        , 0.00465906, 0.00450313, 0.00521239,\n",
       "       0.00727567, 0.00595863, 0.00505911, 0.        , 0.0045165 ,\n",
       "       0.        , 0.00446168, 0.        , 0.00181612, 0.00816894,\n",
       "       0.00674887, 0.00045162, 0.00567352, 0.        , 0.        ,\n",
       "       0.00341504, 0.        , 0.        , 0.00791065, 0.        ,\n",
       "       0.00375689, 0.00717845, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00234956, 0.        , 0.00227289,\n",
       "       0.00424666, 0.        , 0.00522907, 0.00396781, 0.        ,\n",
       "       0.00519866, 0.00551237, 0.00758974, 0.00501979, 0.00378178,\n",
       "       0.        , 0.        , 0.00200536, 0.00194051, 0.00445447,\n",
       "       0.        , 0.00508601, 0.004404  , 0.00271855, 0.        ,\n",
       "       0.00038962, 0.01047682, 0.00428715, 0.00352594, 0.        ,\n",
       "       0.00668763, 0.        , 0.        , 0.01035918, 0.        ,\n",
       "       0.        , 0.0035376 , 0.        , 0.00465791, 0.        ,\n",
       "       0.        , 0.00474872, 0.00526775, 0.        , 0.        ,\n",
       "       0.00427823, 0.00338642, 0.00643852, 0.00410039, 0.00205671,\n",
       "       0.        , 0.00715502, 0.00568529, 0.00215619, 0.        ,\n",
       "       0.        , 0.00494017, 0.00252209, 0.00228821, 0.00397253,\n",
       "       0.        , 0.00271109, 0.0043702 , 0.00410453, 0.0088083 ,\n",
       "       0.        , 0.        , 0.00797309, 0.00718749, 0.00630637,\n",
       "       0.00472293, 0.00478465, 0.        , 0.        , 0.00292133,\n",
       "       0.        , 0.        , 0.0058297 , 0.00657907, 0.        ,\n",
       "       0.        , 0.        , 0.00421386, 0.00534197, 0.01012728,\n",
       "       0.        , 0.00465788, 0.00116697, 0.00381692, 0.00042369,\n",
       "       0.        , 0.00116308, 0.00502522, 0.00818816, 0.00452397,\n",
       "       0.        , 0.00498815, 0.00697268, 0.00715701, 0.        ,\n",
       "       0.00576972, 0.        , 0.00296751, 0.00726286, 0.00283263,\n",
       "       0.        , 0.00528342, 0.00567471, 0.        , 0.00535164,\n",
       "       0.00662674, 0.00357675, 0.        , 0.00425291, 0.00360799,\n",
       "       0.00455811, 0.00546593, 0.00218409, 0.        , 0.00306489,\n",
       "       0.00790099, 0.        , 0.0066972 , 0.00584043, 0.        ,\n",
       "       0.0024707 , 0.        , 0.00252384, 0.00303309, 0.00344639,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00851965,\n",
       "       0.        , 0.00583652, 0.        , 0.00210947, 0.00894857,\n",
       "       0.00472983, 0.        , 0.        , 0.00560365, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00288855, 0.        ,\n",
       "       0.00629676, 0.        , 0.        , 0.        , 0.00148834,\n",
       "       0.        , 0.        , 0.00552056, 0.00504667, 0.        ,\n",
       "       0.        , 0.00208717, 0.00577174, 0.0030254 , 0.        ,\n",
       "       0.        , 0.        , 0.0036516 , 0.        , 0.        ,\n",
       "       0.00255064, 0.00322058, 0.        , 0.        , 0.00475007,\n",
       "       0.00332516, 0.        , 0.00280338, 0.00018563, 0.        ,\n",
       "       0.00557858, 0.        , 0.00327545, 0.        , 0.        ,\n",
       "       0.0028723 , 0.00306356, 0.00379542, 0.00563428, 0.        ,\n",
       "       0.00578314, 0.00649236, 0.00501009, 0.        , 0.0094218 ,\n",
       "       0.006398  , 0.00523948, 0.00868907, 0.00183639, 0.        ,\n",
       "       0.        , 0.01146407, 0.        , 0.00174418, 0.0043491 ,\n",
       "       0.        , 0.        , 0.00447736, 0.00345307, 0.        ,\n",
       "       0.        , 0.        , 0.00484394, 0.00334178, 0.        ,\n",
       "       0.00222035, 0.0056637 , 0.00369401, 0.00586053, 0.        ,\n",
       "       0.        , 0.00262513, 0.        , 0.006537  , 0.00850789,\n",
       "       0.00227913, 0.        , 0.        , 0.        , 0.00595421,\n",
       "       0.        , 0.00701818, 0.00479141, 0.        , 0.00817845,\n",
       "       0.        , 0.00535912, 0.00537361, 0.        , 0.00355192,\n",
       "       0.        , 0.        , 0.00279357, 0.        , 0.        ,\n",
       "       0.        , 0.00693048, 0.        , 0.        , 0.00618363,\n",
       "       0.00806825, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00257824, 0.        , 0.        , 0.        , 0.00292492,\n",
       "       0.        , 0.        , 0.00634626, 0.00638536, 0.00750791,\n",
       "       0.00700294, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00398301, 0.00246268, 0.0082708 , 0.00745023, 0.        ,\n",
       "       0.00134719, 0.00516018, 0.00287003, 0.00576871, 0.00328309,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00388695, 0.00660295, 0.0044865 , 0.00901065, 0.0048322 ,\n",
       "       0.00444955, 0.00386617, 0.        , 0.00402225, 0.        ,\n",
       "       0.        , 0.0064521 , 0.        , 0.01047993, 0.        ,\n",
       "       0.        , 0.        , 0.00362228, 0.0070202 , 0.00389467,\n",
       "       0.00611929, 0.00808877, 0.        , 0.        , 0.00446011,\n",
       "       0.00523494, 0.00350062, 0.0033676 , 0.        , 0.00573162,\n",
       "       0.        , 0.00315492, 0.00253796, 0.0071225 , 0.00268789,\n",
       "       0.0064618 , 0.        , 0.00295611, 0.00547933, 0.00348268,\n",
       "       0.        , 0.        , 0.00443776, 0.00692994, 0.        ,\n",
       "       0.00308431, 0.00740704, 0.00395635, 0.00391762, 0.        ,\n",
       "       0.        , 0.        , 0.00554619, 0.        , 0.00705091,\n",
       "       0.00641499, 0.00305099, 0.00144709, 0.00485388, 0.00416221,\n",
       "       0.00128301, 0.        , 0.00485143, 0.00560847, 0.        ,\n",
       "       0.        , 0.00577291, 0.        , 0.        , 0.0059539 ,\n",
       "       0.00829076, 0.        , 0.00418013, 0.0063088 , 0.00455346,\n",
       "       0.        , 0.        , 0.        , 0.00060306, 0.00071927,\n",
       "       0.00444314, 0.00639684, 0.00824669, 0.        , 0.        ,\n",
       "       0.        , 0.00456444, 0.00635801, 0.        , 0.0079845 ,\n",
       "       0.00703243, 0.        , 0.00630437, 0.        , 0.00456776,\n",
       "       0.00464018, 0.        , 0.00114532, 0.00499245, 0.00039555,\n",
       "       0.00814859, 0.        , 0.00519207, 0.00684149, 0.00617738,\n",
       "       0.        , 0.00184069, 0.        , 0.00621833, 0.00147845,\n",
       "       0.        , 0.        , 0.00188735, 0.        , 0.00478401,\n",
       "       0.        , 0.0070142 , 0.        , 0.        , 0.00479718,\n",
       "       0.00668878, 0.00320286, 0.00390863, 0.00863895, 0.00605209,\n",
       "       0.        , 0.00770767, 0.        , 0.00472535, 0.00404798,\n",
       "       0.00452561, 0.00351343, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00641853, 0.00741878, 0.        , 0.00855138,\n",
       "       0.00441362, 0.00567007, 0.00758523, 0.        , 0.        ,\n",
       "       0.00536815, 0.        , 0.00621282, 0.00507207, 0.        ,\n",
       "       0.00062206, 0.0039978 , 0.0034336 , 0.00322787, 0.        ,\n",
       "       0.        , 0.00140729, 0.00747724, 0.00920822, 0.        ,\n",
       "       0.00638547, 0.00338119, 0.        , 0.00637998, 0.        ,\n",
       "       0.00403288, 0.        , 0.        , 0.00618254, 0.00106692,\n",
       "       0.        , 0.        , 0.        , 0.00640153, 0.        ,\n",
       "       0.00400955, 0.00418717, 0.        , 0.        , 0.00548315,\n",
       "       0.00174831, 0.00236496, 0.        , 0.        , 0.00277953,\n",
       "       0.00391102, 0.00517523, 0.00513331, 0.        , 0.0009844 ,\n",
       "       0.        , 0.        , 0.00577855, 0.        , 0.00929221,\n",
       "       0.        , 0.00425214, 0.00745688, 0.00890285, 0.        ,\n",
       "       0.00892607, 0.00286028, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00552052, 0.00329335, 0.        , 0.0020865 ,\n",
       "       0.        , 0.        , 0.0090713 , 0.        , 0.        ,\n",
       "       0.00467308, 0.00755006, 0.        , 0.        , 0.        ,\n",
       "       0.00647911, 0.00616296, 0.00297879, 0.        , 0.        ,\n",
       "       0.0069378 , 0.00721052, 0.00433811, 0.00894818, 0.        ,\n",
       "       0.00407024, 0.        , 0.00260671, 0.        , 0.00302055,\n",
       "       0.00264475, 0.        , 0.00143611, 0.00158162, 0.        ,\n",
       "       0.00451902, 0.00450726, 0.00253388, 0.        , 0.00689664,\n",
       "       0.00501348, 0.00122982, 0.00700513, 0.        , 0.00767513,\n",
       "       0.00317998, 0.        , 0.00420105, 0.00567085, 0.        ,\n",
       "       0.00440865, 0.00549096, 0.        , 0.        , 0.        ,\n",
       "       0.00212804, 0.        , 0.00315986, 0.        , 0.0085111 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.0019854 , 0.0046504 , 0.00375087, 0.00331147, 0.00538633,\n",
       "       0.00361974, 0.00315004, 0.00784955, 0.        , 0.00190668,\n",
       "       0.        , 0.00431746, 0.00795602, 0.00543101, 0.00335263,\n",
       "       0.        , 0.        , 0.        , 0.00664611, 0.        ,\n",
       "       0.00423365, 0.00372658, 0.        , 0.        , 0.00877514,\n",
       "       0.00088816, 0.00429513, 0.        , 0.00382415, 0.        ,\n",
       "       0.00728652, 0.        , 0.        , 0.        , 0.0106883 ,\n",
       "       0.00453805, 0.        , 0.        , 0.00424425, 0.00604391,\n",
       "       0.00420462, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00282374, 0.00266011, 0.00333962, 0.00419264, 0.        ,\n",
       "       0.00021253, 0.00526926, 0.00594705, 0.00478776, 0.        ,\n",
       "       0.00405094, 0.        , 0.        , 0.00627056, 0.00513453,\n",
       "       0.0035531 , 0.        , 0.0046342 , 0.        , 0.00479752,\n",
       "       0.00546363, 0.01023839, 0.        , 0.00515806, 0.00057661,\n",
       "       0.00300734, 0.00126307, 0.00372114, 0.        , 0.        ,\n",
       "       0.        , 0.00549918, 0.00826508, 0.        , 0.        ,\n",
       "       0.0040955 , 0.00787685, 0.        , 0.00810665, 0.00812513,\n",
       "       0.00757242, 0.        , 0.00308665, 0.00305222, 0.        ,\n",
       "       0.00511541, 0.00320071, 0.        , 0.        , 0.00848158,\n",
       "       0.        , 0.00808183, 0.        , 0.        , 0.        ,\n",
       "       0.00748922, 0.00488615, 0.00612955, 0.        , 0.        ,\n",
       "       0.00790173, 0.00169489, 0.00332467, 0.        , 0.0091862 ,\n",
       "       0.        , 0.00716182, 0.        , 0.        , 0.00391036,\n",
       "       0.00789739, 0.        , 0.00093166, 0.00189848, 0.00723025,\n",
       "       0.00201393, 0.00562008, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0085627 , 0.        , 0.00192285,\n",
       "       0.00281267, 0.00423345, 0.00559151, 0.        , 0.        ,\n",
       "       0.00447621, 0.        , 0.00877758, 0.00448058, 0.        ,\n",
       "       0.        , 0.00070224, 0.00823059, 0.        , 0.00583652,\n",
       "       0.00255087, 0.00488291, 0.        , 0.00312056, 0.        ,\n",
       "       0.00131066, 0.00222171, 0.        , 0.00252965, 0.00537863,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00338429,\n",
       "       0.00471874, 0.00353889, 0.        , 0.00487183, 0.00300393,\n",
       "       0.0049999 , 0.00648608, 0.        , 0.0100872 , 0.00098653,\n",
       "       0.00276121, 0.        , 0.00249956, 0.00101353, 0.0032226 ,\n",
       "       0.00194716, 0.00486581, 0.        , 0.00535647, 0.0024801 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00739376, 0.00813604, 0.        , 0.        ,\n",
       "       0.        , 0.00429903, 0.0070652 , 0.00025212, 0.00272949,\n",
       "       0.        , 0.00220414, 0.00033291, 0.00619909, 0.        ,\n",
       "       0.00377833, 0.00581282, 0.0011215 , 0.00806568, 0.00414135,\n",
       "       0.        , 0.        , 0.00225686, 0.        , 0.00379867,\n",
       "       0.00774855, 0.00148959, 0.        , 0.        , 0.00517082,\n",
       "       0.00520058, 0.00568823, 0.        , 0.00452191, 0.        ,\n",
       "       0.        , 0.00266475, 0.00561279, 0.00265064, 0.00877968,\n",
       "       0.00469604, 0.00318028, 0.00261188, 0.00331351, 0.        ,\n",
       "       0.00307191, 0.00456291, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00321052, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00458678, 0.00417762, 0.00499524, 0.        ,\n",
       "       0.00792782, 0.00703981, 0.        , 0.00529983, 0.        ,\n",
       "       0.        , 0.00661859, 0.00291171, 0.0044429 , 0.00044224,\n",
       "       0.        , 0.0042726 , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = NNGP(model)  # to estimate uncertainties\n",
    "oracle = IdentityOracle(y_pool)  # generate y for X from pool\n",
    "sampler = EagerSampleSelector(oracle) # sample X and y from pool by uncertainty estimations\n",
    "\n",
    "estimator.estimate(sess, X_train, y_train, X_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] RMSE train:186.900 test:191.764 val:191.764 patience:3\n",
      "[200] RMSE train:180.022 test:184.687 val:184.687 patience:3\n",
      "[300] RMSE train:162.864 test:167.031 val:167.031 patience:3\n",
      "[400] RMSE train:131.115 test:134.400 val:134.400 patience:3\n",
      "[500] RMSE train:91.724 test:94.210 val:94.210 patience:3\n",
      "[600] RMSE train:68.533 test:71.244 val:71.244 patience:3\n",
      "[700] RMSE train:64.706 test:67.898 val:67.898 patience:3\n",
      "[800] RMSE train:64.410 test:67.763 val:67.763 patience:3\n",
      "[900] RMSE train:64.252 test:67.672 val:67.672 patience:3\n",
      "[1000] RMSE train:64.087 test:67.564 val:67.564 patience:3\n",
      "[1100] RMSE train:63.915 test:67.451 val:67.451 patience:3\n",
      "[1200] RMSE train:63.736 test:67.334 val:67.334 patience:3\n",
      "[1300] RMSE train:63.551 test:67.214 val:67.214 patience:3\n",
      "[1400] RMSE train:63.361 test:67.090 val:67.090 patience:3\n",
      "[1500] RMSE train:63.167 test:66.965 val:66.965 patience:3\n",
      "[1600] RMSE train:62.969 test:66.837 val:66.837 patience:3\n",
      "[1700] RMSE train:62.768 test:66.709 val:66.709 patience:3\n",
      "[1800] RMSE train:62.564 test:66.580 val:66.580 patience:3\n",
      "[1900] RMSE train:62.359 test:66.451 val:66.451 patience:3\n",
      "[2000] RMSE train:62.154 test:66.322 val:66.322 patience:3\n",
      "[2100] RMSE train:61.949 test:66.195 val:66.195 patience:3\n",
      "[2200] RMSE train:61.744 test:66.070 val:66.070 patience:3\n",
      "[2300] RMSE train:61.542 test:65.948 val:65.948 patience:3\n",
      "[2400] RMSE train:61.342 test:65.829 val:65.829 patience:3\n",
      "[2500] RMSE train:61.146 test:65.714 val:65.714 patience:3\n",
      "[2600] RMSE train:60.954 test:65.603 val:65.603 patience:3\n",
      "[2700] RMSE train:60.768 test:65.498 val:65.498 patience:3\n",
      "[2800] RMSE train:60.588 test:65.398 val:65.398 patience:3\n",
      "[2900] RMSE train:60.415 test:65.305 val:65.305 patience:3\n",
      "[3000] RMSE train:60.249 test:65.219 val:65.219 patience:3\n",
      "[3100] RMSE train:60.090 test:65.139 val:65.139 patience:3\n",
      "[3200] RMSE train:59.938 test:65.067 val:65.067 patience:3\n",
      "[3300] RMSE train:59.794 test:65.001 val:65.001 patience:3\n",
      "[3400] RMSE train:59.659 test:64.941 val:64.941 patience:3\n",
      "[3500] RMSE train:59.533 test:64.888 val:64.888 patience:3\n",
      "[3600] RMSE train:59.415 test:64.843 val:64.843 patience:3\n",
      "[3700] RMSE train:59.304 test:64.804 val:64.804 patience:3\n",
      "[3800] RMSE train:59.202 test:64.766 val:64.766 patience:3\n",
      "[3900] RMSE train:59.106 test:64.731 val:64.731 patience:3\n",
      "[4000] RMSE train:59.017 test:64.695 val:64.695 patience:3\n",
      "[4100] RMSE train:58.933 test:64.661 val:64.661 patience:3\n",
      "[4200] RMSE train:58.851 test:64.626 val:64.626 patience:3\n",
      "[4300] RMSE train:58.766 test:64.588 val:64.588 patience:3\n",
      "[4400] RMSE train:58.688 test:64.554 val:64.554 patience:3\n",
      "[4500] RMSE train:58.611 test:64.517 val:64.517 patience:3\n",
      "[4600] RMSE train:58.531 test:64.472 val:64.472 patience:3\n",
      "[4700] RMSE train:58.449 test:64.426 val:64.426 patience:3\n",
      "[4800] RMSE train:58.363 test:64.386 val:64.386 patience:3\n",
      "[4900] RMSE train:58.259 test:64.317 val:64.317 patience:3\n",
      "[5000] RMSE train:58.149 test:64.260 val:64.260 patience:3\n",
      "[5100] RMSE train:58.031 test:64.183 val:64.183 patience:3\n",
      "[5200] RMSE train:57.916 test:64.102 val:64.102 patience:3\n",
      "[5300] RMSE train:57.800 test:64.022 val:64.022 patience:3\n",
      "[5400] RMSE train:57.679 test:63.936 val:63.936 patience:3\n",
      "[5500] RMSE train:57.551 test:63.843 val:63.843 patience:3\n",
      "[5600] RMSE train:57.417 test:63.743 val:63.743 patience:3\n",
      "[5700] RMSE train:57.269 test:63.642 val:63.642 patience:3\n",
      "[5800] RMSE train:57.118 test:63.522 val:63.522 patience:3\n",
      "[5900] RMSE train:56.968 test:63.403 val:63.403 patience:3\n",
      "[6000] RMSE train:56.805 test:63.270 val:63.270 patience:3\n",
      "[6100] RMSE train:56.614 test:63.100 val:63.100 patience:3\n",
      "[6200] RMSE train:56.392 test:62.905 val:62.905 patience:3\n",
      "[6300] RMSE train:56.172 test:62.735 val:62.735 patience:3\n",
      "[6400] RMSE train:55.930 test:62.552 val:62.552 patience:3\n",
      "[6500] RMSE train:55.622 test:62.304 val:62.304 patience:3\n",
      "[6600] RMSE train:55.228 test:61.960 val:61.960 patience:3\n",
      "[6700] RMSE train:54.781 test:61.643 val:61.643 patience:3\n",
      "[6800] RMSE train:54.263 test:61.226 val:61.226 patience:3\n",
      "[6900] RMSE train:53.705 test:60.766 val:60.766 patience:3\n",
      "[7000] RMSE train:53.036 test:60.237 val:60.237 patience:3\n",
      "[7100] RMSE train:52.135 test:59.519 val:59.519 patience:3\n",
      "[7200] RMSE train:51.022 test:58.651 val:58.651 patience:3\n",
      "[7300] RMSE train:49.687 test:57.641 val:57.641 patience:3\n",
      "[7400] RMSE train:48.207 test:56.397 val:56.397 patience:3\n",
      "[7500] RMSE train:46.507 test:54.936 val:54.936 patience:3\n",
      "[7600] RMSE train:44.627 test:53.373 val:53.373 patience:3\n",
      "[7700] RMSE train:42.567 test:51.682 val:51.682 patience:3\n",
      "[7800] RMSE train:40.010 test:49.539 val:49.539 patience:3\n",
      "[7900] RMSE train:37.684 test:47.573 val:47.573 patience:3\n",
      "[8000] RMSE train:35.075 test:45.503 val:45.503 patience:3\n",
      "[8100] RMSE train:32.748 test:43.691 val:43.691 patience:3\n",
      "[8200] RMSE train:30.406 test:42.010 val:42.010 patience:3\n",
      "[8300] RMSE train:28.039 test:40.624 val:40.624 patience:3\n",
      "[8400] RMSE train:25.493 test:39.126 val:39.126 patience:3\n",
      "[8500] RMSE train:23.421 test:37.991 val:37.991 patience:3\n",
      "[8600] RMSE train:21.556 test:37.102 val:37.102 patience:3\n",
      "[8700] RMSE train:19.753 test:36.321 val:36.321 patience:3\n",
      "[8800] RMSE train:18.422 test:35.835 val:35.835 patience:3\n",
      "[8900] RMSE train:17.280 test:35.458 val:35.458 patience:3\n",
      "[9000] RMSE train:16.292 test:35.086 val:35.086 patience:3\n",
      "[9100] RMSE train:15.340 test:34.806 val:34.806 patience:3\n",
      "[9200] RMSE train:14.490 test:34.517 val:34.517 patience:3\n",
      "[9300] RMSE train:13.733 test:34.265 val:34.265 patience:3\n",
      "[9400] RMSE train:13.050 test:34.148 val:34.148 patience:3\n",
      "[9500] RMSE train:12.383 test:34.032 val:34.032 patience:3\n",
      "[9600] RMSE train:11.765 test:33.965 val:33.965 patience:3\n",
      "[9700] RMSE train:11.176 test:33.907 val:33.907 patience:3\n",
      "[9800] RMSE train:10.652 test:33.866 val:33.866 patience:3\n",
      "[9900] RMSE train:10.192 test:33.839 val:33.839 patience:3\n",
      "[10000] RMSE train:9.696 test:33.796 val:33.796 patience:3\n",
      "[1] BEFORE:\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (1000, 10) (1000, 1)\n",
      "[1] AFTER:\n",
      "shapes: (210, 10) (210, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (990, 10) (1000, 1)\n",
      "[100] RMSE train:12.030 test:32.719 val:32.719 patience:3\n",
      "[200] RMSE train:11.225 test:32.637 val:32.637 patience:3\n",
      "[300] RMSE train:10.596 test:32.674 val:32.674 patience:2\n",
      "[400] RMSE train:10.011 test:32.694 val:32.694 patience:1\n",
      "[500] RMSE train:9.518 test:32.661 val:32.661 patience:0\n",
      "No patience left at epoch 500. Early stopping.\n",
      "[2] BEFORE:\n",
      "shapes: (210, 10) (210, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (990, 10) (1000, 1)\n",
      "[2] AFTER:\n",
      "shapes: (220, 10) (220, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (980, 10) (1000, 1)\n",
      "[100] RMSE train:12.869 test:31.995 val:31.995 patience:3\n",
      "[200] RMSE train:11.886 test:31.564 val:31.564 patience:3\n",
      "[300] RMSE train:11.062 test:31.312 val:31.312 patience:3\n",
      "[400] RMSE train:10.434 test:31.146 val:31.146 patience:3\n",
      "[500] RMSE train:9.816 test:31.014 val:31.014 patience:3\n",
      "[600] RMSE train:9.220 test:31.028 val:31.028 patience:2\n",
      "[700] RMSE train:8.686 test:30.981 val:30.981 patience:3\n",
      "[800] RMSE train:8.242 test:30.960 val:30.960 patience:3\n",
      "[900] RMSE train:7.832 test:30.970 val:30.970 patience:2\n",
      "[1000] RMSE train:7.448 test:31.006 val:31.006 patience:1\n",
      "[1100] RMSE train:7.109 test:31.048 val:31.048 patience:0\n",
      "No patience left at epoch 1100. Early stopping.\n",
      "[3] BEFORE:\n",
      "shapes: (220, 10) (220, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (980, 10) (1000, 1)\n",
      "[3] AFTER:\n",
      "shapes: (230, 10) (230, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (970, 10) (1000, 1)\n",
      "[100] RMSE train:7.671 test:30.959 val:30.959 patience:3\n",
      "[200] RMSE train:7.190 test:31.006 val:31.006 patience:2\n",
      "[300] RMSE train:6.768 test:31.061 val:31.061 patience:1\n",
      "[400] RMSE train:6.379 test:31.098 val:31.098 patience:0\n",
      "No patience left at epoch 400. Early stopping.\n",
      "[4] BEFORE:\n",
      "shapes: (230, 10) (230, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (970, 10) (1000, 1)\n",
      "[4] AFTER:\n",
      "shapes: (240, 10) (240, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (960, 10) (1000, 1)\n",
      "[100] RMSE train:8.331 test:30.617 val:30.617 patience:3\n",
      "[200] RMSE train:7.669 test:30.624 val:30.624 patience:2\n",
      "[300] RMSE train:7.112 test:30.604 val:30.604 patience:3\n",
      "[400] RMSE train:6.661 test:30.550 val:30.550 patience:3\n",
      "[500] RMSE train:6.268 test:30.489 val:30.489 patience:3\n",
      "[600] RMSE train:5.912 test:30.442 val:30.442 patience:3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700] RMSE train:5.563 test:30.420 val:30.420 patience:3\n",
      "[800] RMSE train:5.223 test:30.377 val:30.377 patience:3\n",
      "[900] RMSE train:4.915 test:30.365 val:30.365 patience:3\n",
      "[1000] RMSE train:4.626 test:30.365 val:30.365 patience:3\n",
      "[1100] RMSE train:4.348 test:30.373 val:30.373 patience:2\n",
      "[1200] RMSE train:4.079 test:30.411 val:30.411 patience:1\n",
      "[1300] RMSE train:3.827 test:30.408 val:30.408 patience:0\n",
      "No patience left at epoch 1300. Early stopping.\n",
      "[5] BEFORE:\n",
      "shapes: (240, 10) (240, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (960, 10) (1000, 1)\n",
      "[5] AFTER:\n",
      "shapes: (250, 10) (250, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (950, 10) (1000, 1)\n",
      "[100] RMSE train:4.533 test:30.484 val:30.484 patience:3\n",
      "[200] RMSE train:4.029 test:30.605 val:30.605 patience:2\n",
      "[300] RMSE train:3.676 test:30.689 val:30.689 patience:1\n",
      "[400] RMSE train:3.396 test:30.839 val:30.839 patience:0\n",
      "No patience left at epoch 400. Early stopping.\n",
      "[6] BEFORE:\n",
      "shapes: (250, 10) (250, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (950, 10) (1000, 1)\n",
      "[6] AFTER:\n",
      "shapes: (260, 10) (260, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (940, 10) (1000, 1)\n",
      "[100] RMSE train:4.298 test:30.620 val:30.620 patience:3\n",
      "[200] RMSE train:3.754 test:30.676 val:30.676 patience:2\n",
      "[300] RMSE train:3.356 test:30.775 val:30.775 patience:1\n",
      "[400] RMSE train:3.056 test:30.850 val:30.850 patience:0\n",
      "No patience left at epoch 400. Early stopping.\n",
      "[7] BEFORE:\n",
      "shapes: (260, 10) (260, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (940, 10) (1000, 1)\n",
      "[7] AFTER:\n",
      "shapes: (270, 10) (270, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (930, 10) (1000, 1)\n",
      "[100] RMSE train:4.665 test:30.713 val:30.713 patience:3\n",
      "[200] RMSE train:4.078 test:30.694 val:30.694 patience:3\n",
      "[300] RMSE train:3.672 test:30.618 val:30.618 patience:3\n",
      "[400] RMSE train:3.322 test:30.613 val:30.613 patience:3\n",
      "[500] RMSE train:3.050 test:30.584 val:30.584 patience:3\n",
      "[600] RMSE train:2.820 test:30.537 val:30.537 patience:3\n",
      "[700] RMSE train:2.619 test:30.500 val:30.500 patience:3\n",
      "[800] RMSE train:2.437 test:30.459 val:30.459 patience:3\n",
      "[900] RMSE train:2.272 test:30.437 val:30.437 patience:3\n",
      "[1000] RMSE train:2.129 test:30.378 val:30.378 patience:3\n",
      "[1100] RMSE train:2.003 test:30.365 val:30.365 patience:3\n",
      "[1200] RMSE train:1.889 test:30.363 val:30.363 patience:3\n",
      "[1300] RMSE train:1.785 test:30.375 val:30.375 patience:2\n",
      "[1400] RMSE train:1.696 test:30.392 val:30.392 patience:1\n",
      "[1500] RMSE train:1.613 test:30.396 val:30.396 patience:0\n",
      "No patience left at epoch 1500. Early stopping.\n",
      "[8] BEFORE:\n",
      "shapes: (270, 10) (270, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (930, 10) (1000, 1)\n",
      "[8] AFTER:\n",
      "shapes: (280, 10) (280, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (920, 10) (1000, 1)\n",
      "[100] RMSE train:4.274 test:30.570 val:30.570 patience:3\n",
      "[200] RMSE train:3.803 test:30.594 val:30.594 patience:2\n",
      "[300] RMSE train:3.479 test:30.587 val:30.587 patience:1\n",
      "[400] RMSE train:3.218 test:30.587 val:30.587 patience:0\n",
      "No patience left at epoch 400. Early stopping.\n",
      "[9] BEFORE:\n",
      "shapes: (280, 10) (280, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (920, 10) (1000, 1)\n",
      "[9] AFTER:\n",
      "shapes: (290, 10) (290, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (910, 10) (1000, 1)\n",
      "[100] RMSE train:3.249 test:30.573 val:30.573 patience:3\n",
      "[200] RMSE train:3.007 test:30.605 val:30.605 patience:2\n",
      "[300] RMSE train:2.808 test:30.605 val:30.605 patience:1\n",
      "[400] RMSE train:2.637 test:30.614 val:30.614 patience:0\n",
      "No patience left at epoch 400. Early stopping.\n",
      "[10] BEFORE:\n",
      "shapes: (290, 10) (290, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (910, 10) (1000, 1)\n",
      "[10] AFTER:\n",
      "shapes: (300, 10) (300, 1)\n",
      "shapes: (200, 10) (200, 1)\n",
      "shapes: (900, 10) (1000, 1)\n",
      "[100] RMSE train:7.077 test:29.337 val:29.337 patience:3\n",
      "[200] RMSE train:6.053 test:29.174 val:29.174 patience:3\n",
      "[300] RMSE train:5.441 test:29.174 val:29.174 patience:3\n",
      "[400] RMSE train:4.919 test:29.179 val:29.179 patience:2\n",
      "[500] RMSE train:4.497 test:29.196 val:29.196 patience:1\n",
      "[600] RMSE train:4.142 test:29.204 val:29.204 patience:0\n",
      "No patience left at epoch 600. Early stopping.\n"
     ]
    }
   ],
   "source": [
    "model.train(sess, X_train, y_train, X_test, y_test, X_test, y_test)\n",
    "\n",
    "rmses = [np.sqrt(mse(model.predict(sess, data=X_test), y_test))]\n",
    "\n",
    "\n",
    "for al_iteration in range(1, config['al_iterations']+1):\n",
    "    note = f'[{al_iteration}] BEFORE:'\n",
    "    print_shapes(note, (X_train, y_train), (X_test, y_test), (X_pool, y_pool))\n",
    "    \n",
    "    # update pool\n",
    "    uncertainties = estimator.estimate(sess, X_train, y_train, X_pool)\n",
    "    X_train, y_train, X_pool = sampler.update_sets(\n",
    "        X_train, y_train, X_pool, uncertainties, config['update_sample_size']\n",
    "    )\n",
    "    \n",
    "    note = f'[{al_iteration}] AFTER:'\n",
    "    print_shapes(note, (X_train, y_train), (X_test, y_test), (X_pool, y_pool))\n",
    "    \n",
    "    # retrain net\n",
    "    model.train(sess, X_train, y_train, X_test, y_test, X_test, y_test)\n",
    "    rmses.append(np.sqrt(mse(model.predict(sess, data=X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f069022fe48>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rmses, label='NNGP', marker='.')\n",
    "plt.title('RMS Error by active learning iterations')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[269.82272]\n",
      " [236.60335]\n",
      " [228.05836]]\n",
      "[[269.35414184]\n",
      " [199.67728249]\n",
      " [197.7762695 ]]\n"
     ]
    }
   ],
   "source": [
    "# Show some predictions\n",
    "print(model.predict(sess, data = X_test[:3]))\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([160.86413073, 235.44600944, 191.88563576])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some uncertainties\n",
    "estimator.estimate(sess, X_pool)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
