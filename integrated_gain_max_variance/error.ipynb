{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romaushakov/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "from scipy.optimize import rosen\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes: (10, 10) (10, 1)\n",
      "pool shapes: (15, 10) (15, 1)\n",
      "test shapes: (15, 10) (15, 1)\n"
     ]
    }
   ],
   "source": [
    "df = np.random.uniform(size=(40, 10))\n",
    "targets = rosen(df.T)\n",
    "\n",
    "train_num = 10\n",
    "thres = 25\n",
    "thres2 = 40\n",
    "\n",
    "\n",
    "X_train = df[:train_num]\n",
    "y_train = targets[:train_num][:, None]\n",
    "X_pool = df[train_num:thres]\n",
    "y_pool = targets[train_num:thres][:, None]\n",
    "X_test = df[thres:thres2]\n",
    "y_test = targets[thres:thres2][:, None]\n",
    "print('train shapes:', X_train.shape, y_train.shape)\n",
    "print('pool shapes:', X_pool.shape, y_pool.shape)\n",
    "print('test shapes:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(x_, y_):\n",
    "    return [np.sqrt(mse(x_, y_)), np.mean(np.abs(x_ - y_)), np.max(np.abs(x_ - y_))]\n",
    "\n",
    "def simple_cov(_x, _y):\n",
    "    return np.mean((_x-np.mean(_x))*(_y-np.mean(_y)), axis = 1)\n",
    "\n",
    "def get_mcdues(X):\n",
    "    stds = np.zeros((X.shape[0], T), dtype = float)\n",
    "    for cnt_ in range(T):\n",
    "        stds[:, cnt_] = np.ravel(sess.run(y, feed_dict={x: X, \n",
    "                                                        keep_probability_: .5}))\n",
    "    return np.std(stds, axis = 1)\n",
    "\n",
    "def get_stds(X):\n",
    "    stds = np.zeros((X.shape[0], T), dtype = float)\n",
    "    for cnt_ in range(T):\n",
    "        stds[:, cnt_] = np.ravel(sess.run(y, feed_dict={x: X, \n",
    "                                                        keep_probability_: .5}))\n",
    "    return stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = X_train.shape[1]\n",
    "# layers = [64,32]\n",
    "layers = [64,64,32]\n",
    "\n",
    "learning_rate_decay = .97\n",
    "start_learning_rate = 8e-4\n",
    "learning_rate_schedule_epochs = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$: batch_size $\\times$ dim \n",
    "\n",
    "$W$: dim $\\times$ 1\n",
    "\n",
    "\n",
    "output: batch_size $\\times$ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, [None, ndim])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "learning_rate_ = tf.placeholder(tf.float32)\n",
    "forces_coeff_ = tf.placeholder(tf.float32)\n",
    "keep_probability_ = tf.placeholder(tf.float32, name='keep_probability')\n",
    "l2_reg_ = tf.placeholder(tf.float32, name='l2reg')\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([ndim, layers[0]], stddev=(2/ndim)**.5))\n",
    "b1 = tf.Variable(tf.truncated_normal([layers[0]],  stddev=.1))\n",
    "h1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "h_drop1 = tf.nn.dropout(h1, keep_probability_, noise_shape = [1,layers[0]])\n",
    "\n",
    "Ws = [W1]; bs = [b1]; hs = [h_drop1]\n",
    "for cnt_layer in range(1, len(layers)):\n",
    "    Ws.append(tf.Variable(tf.truncated_normal([layers[cnt_layer - 1], layers[cnt_layer]], \n",
    "                                              stddev=(2/layers[cnt_layer - 1])**.5)))\n",
    "    bs.append(tf.Variable(tf.truncated_normal([layers[cnt_layer]],  stddev=.1)))\n",
    "    hs.append(tf.nn.dropout(tf.nn.relu(tf.matmul(hs[-1], Ws[-1]) + bs[-1]), keep_probability_,\n",
    "                            noise_shape = [1,layers[cnt_layer]]))\n",
    "\n",
    "Ws.append(tf.Variable(tf.truncated_normal([layers[-1], 1], stddev=.1)))\n",
    "bs.append(tf.Variable(tf.truncated_normal([1],  stddev=.1)))\n",
    "\n",
    "# funcs\n",
    "y = tf.matmul(hs[-1], Ws[-1]) + bs[-1]\n",
    "\n",
    "l2_regularizer = sum(tf.nn.l2_loss(Wxxx) for Wxxx in Ws) \n",
    "\n",
    "mse_e = tf.losses.mean_squared_error(predictions = y, labels = y_)\n",
    "loss = mse_e + l2_reg_*l2_regularizer\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(loss)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = start_learning_rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           learning_rate_schedule_epochs, learning_rate_decay, staircase=True)\n",
    "\n",
    "lr_fun = lambda: learning_rate\n",
    "min_lr = lambda: tf.constant(1e-5)\n",
    "actual_lr = tf.case([(tf.less(learning_rate, tf.constant(1e-5)), min_lr)], default=lr_fun)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=actual_lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "epoch = 0\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "init_epochs = 1000000\n",
    "keep_prob = .9\n",
    "l2_reg = 1e-4\n",
    "\n",
    "al_steps = 20\n",
    "uptrain_epochs = 1000000\n",
    "mandatory_uptrain_epochs = 10000\n",
    "sample_each_step = 250\n",
    "T = 25\n",
    "\n",
    "early_stopping_window = .03\n",
    "max_warnings = 3\n",
    "early_stopping_check_step = 100\n",
    "\n",
    "gpnn_max_train = 1000\n",
    "diag_eps = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_current = X_train.copy()\n",
    "y_train_current = y_train.copy()\n",
    "X_pool_current = X_pool.copy()\n",
    "y_pool_current = y_pool.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 8.0000E-04, global step: 0\n"
     ]
    }
   ],
   "source": [
    "lr, gs = sess.run([learning_rate, global_step])\n",
    "print('learning rate: {:.4E}, global step: {}'.format(lr, gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model saved in path: /Users/romaushakov/Desktop/diploma/init_rosenbrock_exp.ckpt\n"
     ]
    }
   ],
   "source": [
    "fname_identifier = \"rosenbrock_exp\"\n",
    "save_path = saver.save(sess, \"~/init_\" + fname_identifier + \".ckpt\")\n",
    "print(\"Init model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/romaushakov/Desktop/diploma/init_rosenbrock_exp.ckpt\n",
      "Init model restored\n"
     ]
    }
   ],
   "source": [
    "fname_identifier = \"rosenbrock_exp\"\n",
    "saver.restore(sess, \"~/init_\" + fname_identifier + \".ckpt\")\n",
    "print(\"Init model restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_current = X_train.copy()\n",
    "y_train_current = y_train.copy()\n",
    "X_pool_current = X_pool.copy()\n",
    "y_pool_current = y_pool.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_block_inv(A_inv, B, C, D):\n",
    "    H = D - C.dot(A_inv).dot(B)\n",
    "    H_inv = 1./ H\n",
    "    a00 = A_inv + H_inv * A_inv.dot(B).dot(C).dot(A_inv)\n",
    "    a01 = -A_inv.dot(B) * H_inv\n",
    "    a10 = -H_inv * C.dot(A_inv)\n",
    "    a11 = H_inv\n",
    "    \n",
    "    return np.block([[a00, a01.reshape(-1, 1)],\n",
    "                    [a10.reshape((1, -1)), np.array(a11).reshape((1, 1))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.eye(3)\n",
    "a[0,0] *= 4\n",
    "b = np.zeros((3, 1))\n",
    "c = np.zeros((1, 3))\n",
    "d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.block([[a, b], [c, d]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25,  0.  ,  0.  , -0.  ],\n",
       "       [ 0.  ,  1.  ,  0.  , -0.  ],\n",
       "       [ 0.  ,  0.  ,  1.  , -0.  ],\n",
       "       [-0.  , -0.  , -0.  ,  1.  ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv = compute_block_inv(np.linalg.inv(a), b, c, d)\n",
    "inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv.dot(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Integral-based ALGO\n",
      "========================================\n",
      "Starting iteration # 0\n"
     ]
    }
   ],
   "source": [
    "gpnn_max_train = 8\n",
    "points_to_integrate = 100\n",
    "\n",
    "print('='*40)\n",
    "print('Integral-based ALGO')\n",
    "print('='*40)\n",
    "\n",
    "for al_iters in range(al_steps):\n",
    "    # 1) get MCDUEs\n",
    "    t = time.time()\n",
    "    print('Starting iteration #', al_iters)\n",
    "    random_train_inds = np.random.permutation(range(len(X_train_current)))[:gpnn_max_train]\n",
    "    random_train_samples = X_train_current[random_train_inds]\n",
    "    \n",
    "    train_and_pool_samples = np.concatenate([random_train_samples, X_pool_current])    \n",
    "    stds = get_stds(train_and_pool_samples)\n",
    "    \n",
    "    K_train_cov = np.cov(stds[:gpnn_max_train, :], ddof = 0)\n",
    "    K_train_cov_inv = np.linalg.inv(K_train_cov + diag_eps * np.eye(gpnn_max_train))\n",
    "    \n",
    "    minimums = random_train_samples.min(axis=0)\n",
    "    maximums = random_train_samples.max(axis=0)\n",
    "    \n",
    "    ### vs are points for integral\n",
    "    vs = np.random.uniform(minimums, maximums,\n",
    "                           size=(points_to_integrate, random_train_samples.shape[1]))\n",
    "    \n",
    "    # get mcdues for random vs\n",
    "    y_vs = get_stds(vs)\n",
    "\n",
    "    ### sigma(v | X) for each v in vs\n",
    "    sigmas = []\n",
    "    for cnt_ in range(len(vs)):\n",
    "        vs_sample = y_vs[cnt_, :]\n",
    "        Q = simple_cov(stds[:gpnn_max_train], vs_sample)[:, None]\n",
    "        KK = np.var(vs_sample)\n",
    "        sigma = KK - np.dot(np.dot(Q.T, K_train_cov_inv), Q)[0][0]\n",
    "        sigmas.append(np.sqrt(sigma))\n",
    "    \n",
    "\n",
    "    # for each x in X_pool_current:\n",
    "    # we count \\int sigma(v|X) - sigma(v|X+x_from_pool) dv\n",
    "    diffs_integral = np.zeros(X_pool_current.shape[0])\n",
    "    \n",
    "    ### extend cov matrix \n",
    "    new_K_cov = np.zeros((gpnn_max_train + 1, gpnn_max_train + 1))\n",
    "    new_K_cov[:gpnn_max_train, :gpnn_max_train] = K_train_cov\n",
    "    \n",
    "    ### loop over pool data\n",
    "    for x_cnt_ in range(len(X_pool_current)):\n",
    "        \n",
    "        # stds was recieved for train_and_pool_samples \n",
    "        # and train_pool_sample = np.concatenate([random_train_sample, X_pool_current])\n",
    "        # and random_train_samples.shape[0] = gpnn_max_train. So\n",
    "        \n",
    "        \n",
    "        # extend cov matrix\n",
    "        # we don't recalculate all cov matrix\n",
    "        # we only add one row \n",
    "        pool_sample = stds[(gpnn_max_train + x_cnt_), :]\n",
    "        Q = simple_cov(stds[:gpnn_max_train, :], pool_sample)[:, None]\n",
    "        Q = Q.ravel()\n",
    "        new_K_cov[-1, :-1] = Q\n",
    "        new_K_cov[:-1, -1] = Q\n",
    "        new_K_cov[-1, -1] = np.var(pool_sample)\n",
    "        new_K_cov_inv = np.linalg.inv(new_K_cov + 0 * diag_eps * np.eye(gpnn_max_train + 1))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145.27525173, -12.05703417,  20.90886227, -56.59646714,\n",
       "          1.38671536, -18.19042247, -52.01671123,  37.26035851,\n",
       "        -46.21952905],\n",
       "       [-12.05703417,  34.98402498,   3.23293147,  -7.96455272,\n",
       "        -13.25495966,   3.96352876,  -7.36031558, -18.78218522,\n",
       "          8.93045956],\n",
       "       [ 20.90886227,   3.23293147,  21.22304131, -15.92775356,\n",
       "        -17.4292095 ,  -0.59915894, -11.99308179,  11.87247775,\n",
       "        -11.95870975],\n",
       "       [-56.59646714,  -7.96455272, -15.92775356,  43.82494725,\n",
       "          9.0960169 ,   2.68758211,  24.45796108, -19.03967278,\n",
       "         18.53530131],\n",
       "       [  1.38671536, -13.25495966, -17.4292095 ,   9.0960169 ,\n",
       "         37.78186798, -14.95915564,   6.90164778,  -3.44596452,\n",
       "          0.16357426],\n",
       "       [-18.19042247,   3.96352876,  -0.59915894,   2.68758211,\n",
       "        -14.95915564,  20.18445458,   4.76417915,  -1.80087189,\n",
       "         -1.95505525],\n",
       "       [-52.01671123,  -7.36031558, -11.99308179,  24.45796108,\n",
       "          6.90164778,   4.76417915,  32.34949885, -12.61279204,\n",
       "          9.93820555],\n",
       "       [ 37.26035851, -18.78218522,  11.87247775, -19.03967278,\n",
       "         -3.44596452,  -1.80087189, -12.61279204,  28.6740723 ,\n",
       "        -16.28407899],\n",
       "       [-46.21952905,   8.93045956, -11.95870975,  18.53530131,\n",
       "          0.16357426,  -1.95505525,   9.93820555, -16.28407899,\n",
       "         35.49643494]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_K_cov_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_K_cov_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 38.22316637,  -1.27812002,   0.21491244, -11.72846534,\n",
       "          1.31271606,  -9.31126634, -16.08939775,   5.3448139 ,\n",
       "          1.09316761],\n",
       "       [ -1.27812002,  21.47233469,   2.96072864,  -7.2663336 ,\n",
       "         -6.46134638,   2.16812879,  -5.55322465, -10.51220043,\n",
       "         -0.32018579],\n",
       "       [  0.21491244,   2.96072864,  11.962279  ,  -4.23450052,\n",
       "        -10.45553044,  -1.92139456,  -3.9951254 ,   3.84812901,\n",
       "          0.40639607],\n",
       "       [-11.72846534,  -7.2663336 ,  -4.23450052,  19.79795955,\n",
       "          3.74940936,   0.73429507,   6.91375405,  -5.35684348,\n",
       "         -0.36673936],\n",
       "       [  1.31271606,  -6.46134638, -10.45553044,   3.74940936,\n",
       "         23.99795049,  -9.40446786,   2.93175022,  -2.2337084 ,\n",
       "          0.06736775],\n",
       "       [ -9.31126634,   2.16812879,  -1.92139456,   0.73429507,\n",
       "         -9.40446786,  13.73875746,   1.1850915 ,  -0.55796432,\n",
       "          0.3077161 ],\n",
       "       [-16.08939775,  -5.55322465,  -3.9951254 ,   6.91375405,\n",
       "          2.93175022,   1.1850915 ,  15.85407339,  -3.2276915 ,\n",
       "          0.03853493],\n",
       "       [  5.3448139 , -10.51220043,   3.84812901,  -5.35684348,\n",
       "         -2.2337084 ,  -0.55796432,  -3.2276915 ,  14.40369123,\n",
       "          0.39990035],\n",
       "       [  1.09316761,  -0.32018579,   0.40639607,  -0.36673936,\n",
       "          0.06736775,   0.3077161 ,   0.03853493,   0.39990035,\n",
       "         -1.65266571]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_block_inv(K_train_cov_inv, Q.reshape((-1, 1)), Q.reshape((1, -1)), np.var(Q) + diag_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53739573, 0.64584942, 0.75697023, 0.46705952, 0.66437434,\n",
       "       0.7597712 , 0.68670603, 0.52919355, 0.66584906])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_K_cov[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53739573, 0.64584942, 0.75697023, 0.46705952, 0.66437434,\n",
       "       0.7597712 , 0.68670603, 0.52919355])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
