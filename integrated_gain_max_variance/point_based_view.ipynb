{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import time\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.cluster import KMeans\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from numba import jit\n",
    "from scipy.optimize import rosen\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "def get_errors(x_, y_):\n",
    "    return [np.sqrt(mse(x_, y_)), np.mean(np.abs(x_ - y_)), np.max(np.abs(x_ - y_))]\n",
    "\n",
    "@jit\n",
    "def simple_cov(_x, _y):\n",
    "    return np.mean((_x-np.mean(_x))*(_y-np.mean(_y)), axis = 1)\n",
    "\n",
    "def get_mcdues(X):\n",
    "    stds = np.zeros((X.shape[0], params['T']), dtype = float)\n",
    "    for cnt_ in range(params['T']):\n",
    "        stds[:, cnt_] = np.ravel(sess.run(y, feed_dict={x: X, \n",
    "                                                        keep_probability_: .5}))\n",
    "    return np.std(stds, axis = 1)\n",
    "\n",
    "def get_stds(X):\n",
    "    stds = np.zeros((X.shape[0], T), dtype = float)\n",
    "    for cnt_ in range(T):\n",
    "        stds[:, cnt_] = np.ravel(sess.run(y, feed_dict={x: X, \n",
    "                                                        keep_probability_: .5}))\n",
    "    return stds\n",
    "\n",
    "@jit\n",
    "def compute_block_inv(A_inv, B, C, D):\n",
    "    H = D - C.dot(A_inv).dot(B)\n",
    "    H_inv = 1./ H\n",
    "    a00 = A_inv + H_inv * A_inv.dot(B).dot(C).dot(A_inv)\n",
    "    a01 = -A_inv.dot(B) * H_inv\n",
    "    a10 = -H_inv * C.dot(A_inv)\n",
    "    a11 = H_inv\n",
    "    \n",
    "    return np.block([[a00, a01.reshape(-1, 1)],\n",
    "                    [a10.reshape((1, -1)), a11[0]]])\n",
    "\n",
    "params = pd.read_pickle(\"params.pickle\")\n",
    "\n",
    "df = pd.read_csv('slice_localization_data.csv')\n",
    "df.drop(['patientId'], axis = 1, inplace = True)\n",
    "target_label = 'reference'\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True) # TODO: save these indices\n",
    "targets = df[target_label].values    \n",
    "df.drop([target_label], axis = 1, inplace = True)\n",
    "\n",
    "N_total = len(df)\n",
    "df = df.iloc[params['df_index']]\n",
    "N_train, N_test, N_val, N_pool = params['N_train'], params['N_test'], params['N_val'], params['N_pool']\n",
    "\n",
    "\n",
    "X_train = df[:N_train].values\n",
    "y_train = targets[:N_train][:, None]\n",
    "X_test = df[N_train:(N_train + N_test)].values\n",
    "y_test = targets[N_train:(N_train + N_test)][:, None]\n",
    "X_pool = df[-N_pool:].values\n",
    "y_pool = targets[-N_pool:][:, None]\n",
    "X_val = df[-(N_val+N_pool):-N_pool].values\n",
    "y_val = targets[-(N_val+N_pool):-N_pool][:, None]\n",
    "\n",
    "print('shapes:', X_train.shape, y_train.shape)\n",
    "print('shapes:', X_test.shape, y_train.shape)\n",
    "print('shapes:', X_pool.shape, y_pool.shape)\n",
    "print('shapes:', X_val.shape, y_val.shape)\n",
    "\n",
    "\n",
    "ndim = params['ndim']\n",
    "# layers = [64,32]\n",
    "layers = params['layers']\n",
    "\n",
    "learning_rate_decay = params['learning_rate_decay']\n",
    "start_learning_rate = params['start_learning_rate']\n",
    "learning_rate_schedule_epochs = params['learning_rate_schedule_epochs']\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, [None, ndim])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "learning_rate_ = tf.placeholder(tf.float32)\n",
    "forces_coeff_ = tf.placeholder(tf.float32)\n",
    "keep_probability_ = tf.placeholder(tf.float32, name='keep_probability')\n",
    "l2_reg_ = tf.placeholder(tf.float32, name='l2reg')\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([ndim, layers[0]], stddev=(2/ndim)**.5))\n",
    "b1 = tf.Variable(tf.truncated_normal([layers[0]],  stddev=.1))\n",
    "h1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "h_drop1 = tf.nn.dropout(h1, keep_probability_, noise_shape = [1,layers[0]])\n",
    "\n",
    "Ws = [W1]; bs = [b1]; hs = [h_drop1]\n",
    "for cnt_layer in range(1, len(layers)):\n",
    "    Ws.append(tf.Variable(tf.truncated_normal([layers[cnt_layer - 1], layers[cnt_layer]], \n",
    "                                              stddev=(2/layers[cnt_layer - 1])**.5)))\n",
    "    bs.append(tf.Variable(tf.truncated_normal([layers[cnt_layer]],  stddev=.1)))\n",
    "    hs.append(tf.nn.dropout(tf.nn.relu(tf.matmul(hs[-1], Ws[-1]) + bs[-1]), keep_probability_,\n",
    "                            noise_shape = [1,layers[cnt_layer]]))\n",
    "\n",
    "Ws.append(tf.Variable(tf.truncated_normal([layers[-1], 1], stddev=.1)))\n",
    "bs.append(tf.Variable(tf.truncated_normal([1],  stddev=.1)))\n",
    "\n",
    "# funcs\n",
    "y = tf.matmul(hs[-1], Ws[-1]) + bs[-1]\n",
    "\n",
    "l2_regularizer = sum(tf.nn.l2_loss(Wxxx) for Wxxx in Ws) \n",
    "\n",
    "mse_e = tf.losses.mean_squared_error(predictions = y, labels = y_)\n",
    "loss = mse_e + l2_reg_*l2_regularizer\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(loss)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = start_learning_rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           learning_rate_schedule_epochs, learning_rate_decay, staircase=True)\n",
    "\n",
    "lr_fun = lambda: learning_rate\n",
    "min_lr = lambda: tf.constant(1e-5)\n",
    "actual_lr = tf.case([(tf.less(learning_rate, tf.constant(1e-5)), min_lr)], default=lr_fun)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=actual_lr).minimize(loss, global_step=global_step)\n",
    "\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "epoch = 0\n",
    "data = []\n",
    "\n",
    "batch_size = params['batch_size']\n",
    "init_epochs = params['init_epochs']\n",
    "keep_prob = params['keep_prob']\n",
    "l2_reg = params['l2_reg']\n",
    "\n",
    "al_steps = params['al_steps']\n",
    "uptrain_epochs = params['uptrain_epochs']\n",
    "mandatory_uptrain_epochs = params['mandatory_uptrain_epochs']\n",
    "sample_each_step = params['sample_each_step']\n",
    "T = params['T']\n",
    "\n",
    "early_stopping_window = params['early_stopping_window']\n",
    "max_warnings = params['max_warnings']\n",
    "early_stopping_check_step = params['early_stopping_check_step']\n",
    "\n",
    "gpnn_max_train = params['gpnn_max_train']\n",
    "diag_eps = params['diag_eps']\n",
    "\n",
    "data_columns = params['data_columns']\n",
    "\n",
    "X_train_current = X_train.copy()\n",
    "y_train_current = y_train.copy()\n",
    "X_pool_current = X_pool.copy()\n",
    "y_pool_current = y_pool.copy()\n",
    "\n",
    "fname_identifier = params['fname_identifier']\n",
    "w8s_folder = \"/Users/r.ushakov/Desktop/NNGP/integrated_gain_max_variance/init_w8s/\"\n",
    "saver.restore(sess, w8s_folder + \"init_\" + params['fname_identifier'] + \".ckpt\")\n",
    "print(\"Init model restored\")\n",
    "\n",
    "@jit\n",
    "def dodot(Q, W):\n",
    "    return np.dot(np.dot(Q.T, W), Q)[0][0]\n",
    "\n",
    "@jit\n",
    "def fxjit(x_cnt_):\n",
    "    pool_sample = stds[(gpnn_max_train + x_cnt_), :]\n",
    "    #t = time.time()\n",
    "    Q = simple_cov(stds[:gpnn_max_train, :], pool_sample)[:, None]\n",
    "    Q = Q.ravel()\n",
    "    #print('Q in', np.round(time.time()-t, 2)); t=time.time()\n",
    "    new_K_cov[-1, :-1] = Q\n",
    "    new_K_cov[:-1, -1] = Q\n",
    "    new_K_cov[-1, -1] = np.var(pool_sample)\n",
    "    #print('Q insert in', np.round(time.time()-t, 2)); t=time.time()\n",
    "    new_K_cov_inv = compute_block_inv(K_train_cov_inv,\n",
    "                                      Q.reshape((-1, 1)),\n",
    "                                      Q.reshape((1, -1)), \n",
    "                                      np.var(pool_sample) + 0.001)\n",
    "    #print('new_K_cov_inv in', np.round(time.time()-t, 2)); t=time.time()\n",
    "    indices = list(range(gpnn_max_train)) + [gpnn_max_train + x_cnt_]\n",
    "    si = stds[indices]\n",
    "    ### count sigma(v | X + x_from_pool) with extended \n",
    "    ### cov matrix for each v in vs\n",
    "    extended_sigmas = np.zeros((len(y_vs),))\n",
    "    for cnt_ in range(len(y_vs)):\n",
    "        vs_sample = y_vs[cnt_, :]\n",
    "        Q = simple_cov(si, vs_sample)[:, None]\n",
    "        KK = np.var(vs_sample)\n",
    "        #sigma = KK + params['diag_eps'] - np.dot(np.dot(Q.T, new_K_cov_inv), Q)[0][0]\n",
    "        sigma = KK - dodot(Q, new_K_cov_inv) + diag_eps\n",
    "        extended_sigmas[cnt_] = np.sqrt(sigma)\n",
    "    return extended_sigmas\n",
    "#     #print('integrate in', np.round(time.time()-t, 2)); t=time.time()\n",
    "#     current_diff = np.array(sigmas) - np.array(extended_sigmas)\n",
    "#     #print('misc in', np.round(time.time()-t, 2)); t=time.time()\n",
    "#     return current_diff.sum()\n",
    "\n",
    "extended_sigmas_list = []\n",
    "sigmas_list = []\n",
    "np.random.seed(0)\n",
    "\n",
    "points_int_list = [10, 50, 100, 200, 300]\n",
    "\n",
    "for points_to_integrate in points_int_list:\n",
    "\n",
    "    t = time.time()\n",
    "    perm = np.random.permutation(range(len(X_train_current)))\n",
    "    random_train_inds = perm[:gpnn_max_train]\n",
    "    random_train_samples = X_train_current[random_train_inds]\n",
    "    train_and_pool_samples = np.concatenate([random_train_samples, X_pool_current])    \n",
    "    stds = get_stds(train_and_pool_samples)\n",
    "    K_train_cov = np.cov(stds[:gpnn_max_train, :], ddof = 0)\n",
    "    K_train_cov_inv = np.linalg.inv(K_train_cov + diag_eps * np.eye(gpnn_max_train))\n",
    "\n",
    "    vs = X_pool_current[-points_to_integrate:,:]\n",
    "    y_vs = get_stds(vs)\n",
    "#     print('Work on sigmas at #', al_iters)\n",
    "    ### sigma(v | X) for each v in vs\n",
    "    sigmas = np.zeros((len(y_vs),))\n",
    "    for cnt_ in range(len(vs)):\n",
    "        vs_sample = y_vs[cnt_, :]\n",
    "        Q = simple_cov(stds[:gpnn_max_train], vs_sample)[:, None]\n",
    "        KK = np.var(vs_sample)\n",
    "        sigma = KK - np.dot(np.dot(Q.T, K_train_cov_inv), Q)[0][0]\n",
    "        sigmas[cnt_] = np.sqrt(sigma)\n",
    "\n",
    "    diffs_integral = np.zeros(X_pool_current.shape[0])\n",
    "    new_K_cov = np.zeros((gpnn_max_train + 1, gpnn_max_train + 1))\n",
    "    new_K_cov[:gpnn_max_train, :gpnn_max_train] = K_train_cov\n",
    "\n",
    "    for x_cnt_ in range(len(X_pool_current)-points_to_integrate):\n",
    "        extended_sigmas = fxjit(x_cnt_)\n",
    "        if x_cnt_ % 10 == 0:\n",
    "            print(x_cnt_, end = '|')\n",
    "            \n",
    "    sigmas_list.append(sigmas)\n",
    "    extended_sigmas_list.append(extended_sigmas)\n",
    "    \n",
    "for i in range(7):\n",
    "    idx_point = i\n",
    "    ext_point_idx_values = [arr[idx_point] for arr in extended_sigmas_list]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(points_int_list, ext_point_idx_values, label=\"UE for point {}\".format(idx_point))\n",
    "    plt.xlabel(\"points to integrate\")\n",
    "    plt.ylabel(\"ue\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(\"ue_point_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
