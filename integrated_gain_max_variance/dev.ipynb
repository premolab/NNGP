{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romaushakov/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "from scipy.optimize import rosen\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes: (500, 10) (500, 1)\n",
      "pool shapes: (3000, 10) (3000, 1)\n",
      "test shapes: (500, 10) (500, 1)\n"
     ]
    }
   ],
   "source": [
    "df = np.random.uniform(size=(4000, 10))\n",
    "targets = rosen(df.T)\n",
    "\n",
    "train_num = 500\n",
    "thres = 3500\n",
    "thres2 = 4000\n",
    "\n",
    "\n",
    "X_train = df[:train_num]\n",
    "y_train = targets[:train_num][:, None]\n",
    "X_pool = df[train_num:thres]\n",
    "y_pool = targets[train_num:thres][:, None]\n",
    "X_test = df[thres:thres2]\n",
    "y_test = targets[thres:thres2][:, None]\n",
    "print('train shapes:', X_train.shape, y_train.shape)\n",
    "print('pool shapes:', X_pool.shape, y_pool.shape)\n",
    "print('test shapes:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(x_, y_):\n",
    "    return [np.sqrt(mse(x_, y_)), np.mean(np.abs(x_ - y_)), np.max(np.abs(x_ - y_))]\n",
    "\n",
    "def simple_cov(_x, _y):\n",
    "    return np.mean((_x-np.mean(_x))*(_y-np.mean(_y)), axis = 1)\n",
    "\n",
    "def get_mcdues(X):\n",
    "    stds = np.zeros((X.shape[0], T), dtype = float)\n",
    "    for cnt_ in range(T):\n",
    "        stds[:, cnt_] = np.ravel(sess.run(y, feed_dict={x: X, \n",
    "                                                        keep_probability_: .5}))\n",
    "    return np.std(stds, axis = 1)\n",
    "\n",
    "def get_stds(X):\n",
    "    stds = np.zeros((X.shape[0], T), dtype = float)\n",
    "    for cnt_ in range(T):\n",
    "        stds[:, cnt_] = np.ravel(sess.run(y, feed_dict={x: X, \n",
    "                                                        keep_probability_: .5}))\n",
    "    return stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = X_train.shape[1]\n",
    "# layers = [64,32]\n",
    "layers = [64,64,32]\n",
    "\n",
    "learning_rate_decay = .97\n",
    "start_learning_rate = 8e-4\n",
    "learning_rate_schedule_epochs = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$: batch_size $\\times$ dim \n",
    "\n",
    "$W$: dim $\\times$ 1\n",
    "\n",
    "\n",
    "output: batch_size $\\times$ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, [None, ndim])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "learning_rate_ = tf.placeholder(tf.float32)\n",
    "forces_coeff_ = tf.placeholder(tf.float32)\n",
    "keep_probability_ = tf.placeholder(tf.float32, name='keep_probability')\n",
    "l2_reg_ = tf.placeholder(tf.float32, name='l2reg')\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([ndim, layers[0]], stddev=(2/ndim)**.5))\n",
    "b1 = tf.Variable(tf.truncated_normal([layers[0]],  stddev=.1))\n",
    "h1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "h_drop1 = tf.nn.dropout(h1, keep_probability_, noise_shape = [1,layers[0]])\n",
    "\n",
    "Ws = [W1]; bs = [b1]; hs = [h_drop1]\n",
    "for cnt_layer in range(1, len(layers)):\n",
    "    Ws.append(tf.Variable(tf.truncated_normal([layers[cnt_layer - 1], layers[cnt_layer]], \n",
    "                                              stddev=(2/layers[cnt_layer - 1])**.5)))\n",
    "    bs.append(tf.Variable(tf.truncated_normal([layers[cnt_layer]],  stddev=.1)))\n",
    "    hs.append(tf.nn.dropout(tf.nn.relu(tf.matmul(hs[-1], Ws[-1]) + bs[-1]), keep_probability_,\n",
    "                            noise_shape = [1,layers[cnt_layer]]))\n",
    "\n",
    "Ws.append(tf.Variable(tf.truncated_normal([layers[-1], 1], stddev=.1)))\n",
    "bs.append(tf.Variable(tf.truncated_normal([1],  stddev=.1)))\n",
    "\n",
    "# funcs\n",
    "y = tf.matmul(hs[-1], Ws[-1]) + bs[-1]\n",
    "\n",
    "l2_regularizer = sum(tf.nn.l2_loss(Wxxx) for Wxxx in Ws) \n",
    "\n",
    "mse_e = tf.losses.mean_squared_error(predictions = y, labels = y_)\n",
    "loss = mse_e + l2_reg_*l2_regularizer\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(loss)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = start_learning_rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           learning_rate_schedule_epochs, learning_rate_decay, staircase=True)\n",
    "\n",
    "lr_fun = lambda: learning_rate\n",
    "min_lr = lambda: tf.constant(1e-5)\n",
    "actual_lr = tf.case([(tf.less(learning_rate, tf.constant(1e-5)), min_lr)], default=lr_fun)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=actual_lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "epoch = 0\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "init_epochs = 50000\n",
    "keep_prob = .95\n",
    "l2_reg = 5e-5\n",
    "\n",
    "al_steps = 20\n",
    "uptrain_epochs = 1000000\n",
    "mandatory_uptrain_epochs = 10000\n",
    "sample_each_step = 250\n",
    "T = 25\n",
    "\n",
    "early_stopping_window = .03\n",
    "max_warnings = 3\n",
    "early_stopping_check_step = 100\n",
    "\n",
    "gpnn_max_train = 1000\n",
    "diag_eps = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_current = X_train.copy()\n",
    "y_train_current = y_train.copy()\n",
    "X_pool_current = X_pool.copy()\n",
    "y_pool_current = y_pool.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Initial_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 8.0000E-04, global step: 0\n",
      "0.32s & 0.02 s\n",
      "100 [135.9093 120.0631 357.4479] [135.3936 118.7645 401.5934]|0.17s & 0.0 s\n",
      "200 [ 67.3443  52.8181 232.7036] [ 67.9493  53.1643 268.4205]|0.2s & 0.0 s\n",
      "300 [ 66.2611  52.0387 230.3679] [ 67.0551  52.5301 266.5564]|0.16s & 0.0 s\n",
      "400 [ 65.1467  51.0979 231.3316] [ 65.9954  51.5933 268.1848]|0.17s & 0.0 s\n",
      "500 [ 64.2338  50.6414 222.3337] [ 65.3265  51.5736 258.2432]|0.18s & 0.0 s\n",
      "600 [ 62.8985  49.0971 229.8245] [ 63.7656  49.6281 267.4427]|0.21s & 0.0 s\n",
      "700 [ 61.032   47.953  219.2614] [ 62.1037  48.8993 255.9033]|0.15s & 0.0 s\n",
      "800 [ 59.1246  46.5482 211.0869] [ 60.2686  47.729  247.8392]|0.17s & 0.0 s\n",
      "900 [ 56.9568  45.2552 195.6083] [ 58.3908  46.918  231.4352]|0.16s & 0.0 s\n",
      "1000 [ 54.1451  42.0308 198.0736] [ 54.9259  42.8234 236.2742]|0.18s & 0.0 s\n",
      "1100 [ 50.0476  39.6816 180.2931] [ 51.3083  40.8384 211.8258]|0.19s & 0.0 s\n",
      "1200 [ 46.7456  37.4673 173.9258] [ 48.2497  38.4745 192.6955]|0.19s & 0.0 s\n",
      "1300 [ 43.6563  34.8658 169.8586] [ 45.0677  35.6188 176.3898]|0.16s & 0.0 s\n",
      "1400 [ 41.456   32.7344 164.0729] [ 42.592   33.2749 165.1197]|0.17s & 0.0 s\n",
      "1500 [ 38.7428  31.7721 148.0598] [ 41.0724  33.0078 142.6756]|0.17s & 0.0 s\n",
      "1600 [ 36.9483  29.4165 145.6015] [ 38.9426  30.5906 140.5539]|0.18s & 0.0 s\n",
      "1700 [ 35.1497  28.3053 134.761 ] [ 37.7004  29.8098 130.7105]|0.16s & 0.0 s\n",
      "1800 [ 33.6846  27.3993 125.2834] [ 37.0365  29.6912 117.2083]|0.22s & 0.0 s\n",
      "1900 [ 33.0789  26.3064 124.3617] [ 35.8371  28.0576 124.8989]|0.16s & 0.0 s\n",
      "2000 [ 32.1188  25.4455 117.9138] [ 35.0414  27.3556 125.9514]|0.17s & 0.0 s\n",
      "2100 [ 31.5431  24.8076 112.3529] [ 34.4309  26.6692 122.6106]|0.16s & 0.0 s\n",
      "2200 [ 29.8327  23.7532 102.9811] [ 33.5607  26.2782 116.2891]|0.17s & 0.0 s\n",
      "2300 [ 29.1086  22.9939 100.7436] [ 32.8389  25.6511 116.6376]|0.18s & 0.0 s\n",
      "2400 [ 30.2059  23.4803 101.3711] [ 33.1833  25.4868 120.4517]|0.19s & 0.0 s\n",
      "2500 [28.0316 21.9818 93.9016] [ 32.0373  24.8966 114.2715]|0.18s & 0.0 s\n",
      "2600 [26.663  21.1015 86.2728] [ 31.6376  24.8125 107.9737]|0.2s & 0.0 s\n",
      "2700 [25.7806 20.6782 81.0236] [ 31.719   25.2894 101.589 ]|0.18s & 0.0 s\n",
      "2800 [25.9115 20.3164 90.7964] [ 30.7775  23.8649 107.4606]|0.17s & 0.0 s\n",
      "2900 [24.9738 19.759  84.7094] [ 30.576   23.8959 103.9856]|0.15s & 0.0 s\n",
      "3000 [26.8544 20.9542 93.9723] [ 30.9548  23.6632 109.1819]|0.16s & 0.0 s\n",
      "3100 [26.1242 21.3511 68.2351] [33.995  28.1618 85.0829]|0.17s & 0.0 s\n",
      "3200 [24.1773 19.5862 72.6268] [31.7162 25.8306 90.5193]|0.18s & 0.0 s\n",
      "3300 [22.6272 18.1151 76.9889] [29.9282 23.905  93.2352]|0.16s & 0.0 s\n",
      "3400 [22.5501 18.0995 70.3419] [30.0506 24.1806 88.1845]|0.16s & 0.0 s\n",
      "3500 [26.6643 20.9432 94.547 ] [ 30.713   23.1452 108.7989]|0.19s & 0.0 s\n",
      "3600 [21.7733 17.4954 73.9542] [29.8134 23.9965 88.9208]|0.18s & 0.0 s\n",
      "3700 [21.1721 16.9229 75.1332] [28.8536 22.8765 89.8587]|0.18s & 0.0 s\n",
      "3800 [23.0895 18.7165 63.525 ] [31.5207 25.778  79.4451]|0.18s & 0.0 s\n",
      "3900 [20.5514 16.369  76.0474] [28.2397 22.2113 90.5647]|0.18s & 0.0 s\n",
      "4000 [20.9041 16.4457 82.6415] [27.7766 21.5103 91.8939]|0.19s & 0.0 s\n",
      "4100 [20.6336 16.2642 81.7609] [27.6983 21.4564 92.3551]|0.17s & 0.0 s\n",
      "4200 [20.8953 16.3974 86.5264] [27.7517 21.3903 96.5379]|0.18s & 0.0 s\n",
      "4300 [21.964  17.1037 87.5082] [27.7688 21.1543 96.3282]|0.18s & 0.0 s\n",
      "4400 [19.718  15.8114 69.1914] [28.3466 22.6357 81.4829]|0.19s & 0.0 s\n",
      "4500 [20.2027 15.9448 78.0791] [27.2289 20.8951 89.5737]|0.18s & 0.0 s\n",
      "4600 [19.0447 15.1984 71.9851] [27.6343 21.7971 83.7786]|0.18s & 0.0 s\n",
      "4700 [19.0629 15.1896 72.0981] [27.0739 21.1058 86.5405]|0.16s & 0.0 s\n",
      "4800 [20.0024 15.7445 83.0734] [26.9112 20.6427 90.1654]|0.2s & 0.0 s\n",
      "4900 [18.8536 15.129  65.5923] [28.2085 22.5259 78.5706]|0.17s & 0.0 s\n",
      "5000 [18.3871 14.553  72.26  ] [26.3989 20.6062 83.2724]|0.19s & 0.0 s\n",
      "5100 [17.9859 14.2918 71.2211] [26.5958 20.8199 83.0005]|0.16s & 0.0 s\n",
      "5200 [19.929  15.6127 80.5327] [26.5873 20.3775 90.2624]|0.18s & 0.0 s\n",
      "5300 [17.667  14.035  72.6453] [26.5562 20.8524 82.8371]|0.18s & 0.0 s\n",
      "5400 [17.8049 14.1121 71.4531] [26.1184 20.333  84.8405]|0.19s & 0.0 s\n",
      "5500 [17.658  14.0464 66.6454] [26.0354 20.29   82.7988]|0.17s & 0.0 s\n",
      "5600 [17.8677 14.3312 57.2775] [27.3958 21.7736 77.9558]|0.21s & 0.0 s\n",
      "5700 [23.1976 19.1856 64.5587] [32.7107 27.1758 82.4825]|0.17s & 0.0 s\n",
      "5800 [18.6855 14.9775 56.2706] [28.7866 23.2237 76.7003]|0.18s & 0.0 s\n",
      "5900 [16.711  13.2942 63.2369] [26.1617 20.5022 81.5343]|0.16s & 0.0 s\n",
      "6000 [16.8884 13.4385 66.0077] [25.9297 20.2053 82.6896]|0.18s & 0.0 s\n",
      "6100 [16.8258 13.3777 67.5272] [25.8234 20.1086 83.0177]|0.17s & 0.0 s\n",
      "6200 [17.251  13.5607 68.4191] [25.4565 19.7076 85.1773]|0.16s & 0.0 s\n",
      "6300 [16.3433 12.9576 61.3591] [26.2676 20.667  75.7691]|0.17s & 0.0 s\n",
      "6400 [17.1656 13.7431 52.927 ] [27.3599 21.9394 74.0928]|0.2s & 0.0 s\n",
      "6500 [16.7289 13.3977 52.298 ] [26.8502 21.4577 74.4453]|0.17s & 0.0 s\n",
      "6600 [15.8564 12.6752 59.7641] [25.564  20.0868 80.7202]|0.18s & 0.0 s\n",
      "6700 [16.587  13.1159 66.4671] [25.3518 19.6054 84.0232]|0.17s & 0.0 s\n",
      "6800 [16.9669 13.4095 64.8726] [25.3819 19.5349 85.3702]|0.2s & 0.0 s\n",
      "6900 [16.5608 13.2936 51.8985] [26.9069 21.4732 74.6617]|0.18s & 0.0 s\n",
      "7000 [15.6695 12.5449 57.0919] [25.3787 19.9409 80.4763]|0.19s & 0.0 s\n",
      "7100 [18.8856 14.7231 71.0027] [25.945  19.7299 89.1259]|0.17s & 0.0 s\n",
      "7200 [19.5996 15.3696 71.2661] [26.297  19.9648 91.8586]|0.19s & 0.0 s\n",
      "7300 [15.8606 12.5399 62.922 ] [25.115  19.3899 83.4459]|0.18s & 0.0 s\n",
      "7400 [16.2206 12.7363 59.7138] [24.8586 19.1833 83.8139]|0.19s & 0.0 s\n",
      "7500 [15.9579 12.8319 47.6622] [26.6882 21.2444 76.0756]|0.18s & 0.0 s\n",
      "7600 [17.3792 14.0538 51.0181] [28.1737 22.8125 79.9009]|0.19s & 0.0 s\n",
      "7700 [16.3889 12.8509 62.4861] [25.0073 19.178  87.995 ]|0.18s & 0.0 s\n",
      "7800 [16.0099 12.5748 60.8637] [24.9876 19.2073 87.8349]|0.19s & 0.0 s\n",
      "7900 [16.1853 12.6696 62.2861] [25.0557 19.2099 88.3714]|0.22s & 0.0 s\n",
      "8000 [16.3862 12.7372 60.7981] [25.0077 19.0797 87.2226]|0.18s & 0.0 s\n",
      "8100 [14.4347 11.4817 56.6513] [25.1317 19.6759 80.4009]|0.2s & 0.0 s\n",
      "8200 [16.4962 12.8418 64.0247] [25.0815 19.1236 88.9836]|0.18s & 0.0 s\n",
      "8300 [17.8565 13.9046 65.9584] [25.4696 19.2863 91.0367]|0.18s & 0.0 s\n",
      "8400 [15.6037 12.2176 59.9537] [24.8368 19.079  88.2869]|0.19s & 0.0 s\n",
      "8500 [17.4243 13.6033 61.6215] [25.2153 19.1559 91.9208]|0.17s & 0.0 s\n",
      "8600 [14.572  11.4666 60.4943] [24.7584 19.1694 80.6699]|0.15s & 0.0 s\n",
      "8700 [14.1219 11.344  45.8798] [25.4149 20.0181 78.9858]|0.16s & 0.0 s\n",
      "8800 [14.9397 11.9758 47.5204] [26.4461 21.0007 76.9177]|0.2s & 0.0 s\n",
      "8900 [14.2131 11.4079 45.1447] [25.5988 20.2755 78.5219]|0.17s & 0.0 s\n",
      "9000 [15.1602 12.137  46.4815] [26.6498 21.2963 77.88  ]|0.16s & 0.0 s\n",
      "9100 [14.6541 11.454  56.0767] [24.3536 18.7715 86.0163]|0.18s & 0.0 s\n",
      "9200 [13.8042 11.0235 44.9807] [25.3108 20.0733 79.1994]|0.19s & 0.0 s\n",
      "9300 [14.0313 11.0313 56.1846] [24.3668 18.9782 84.4992]|0.17s & 0.0 s\n",
      "9400 [17.5104 13.641  64.2699] [25.2653 19.1252 91.3978]|0.15s & 0.0 s\n",
      "9500 [15.4338 12.0395 62.3587] [24.5941 18.7184 87.6172]|0.16s & 0.0 s\n",
      "9600 [13.4793 10.6845 48.825 ] [24.5089 19.1559 81.856 ]|0.2s & 0.0 s\n",
      "9700 [13.6578 10.9346 42.7419] [24.9687 19.8138 77.3425]|0.17s & 0.0 s\n",
      "9800 [18.3604 15.1894 52.5819] [29.4708 24.1012 80.3603]|0.16s & 0.0 s\n",
      "9900 [13.9216 10.9106 56.389 ] [24.1986 18.7035 84.2275]|0.17s & 0.0 s\n",
      "10000 [14.1079 11.2655 43.1855] [25.8755 20.6221 77.9613]|0.19s & 0.0 s\n",
      "10100 [13.3399 10.6802 42.4518] [25.1186 19.8568 81.79  ]|*||0.17s & 0.0 s\n",
      "10200 [14.2827 11.4357 45.3649] [26.1962 20.8446 78.2298]|**||0.19s & 0.0 s\n",
      "10300 [13.564  10.5983 50.3129] [24.0727 18.6216 87.5886]|0.22s & 0.0 s\n",
      "10400 [13.2041 10.4025 49.508 ] [24.1423 18.7345 85.8082]|0.17s & 0.0 s\n",
      "10500 [15.4563 12.0734 56.3279] [24.467  18.5678 90.0885]|0.19s & 0.0 s\n",
      "10600 [22.5613 18.6384 71.5968] [27.9903 21.3883 99.9626]|*||0.17s & 0.0 s\n",
      "10700 [13.2212 10.3918 50.5965] [24.0703 18.5134 85.0731]|0.18s & 0.0 s\n",
      "10800 [13.6444 10.922  41.1088] [25.9525 20.573  78.002 ]|*||0.19s & 0.0 s\n",
      "10900 [13.0469 10.1937 50.336 ] [24.0371 18.5047 85.7541]|0.17s & 0.0 s\n",
      "11000 [12.572  10.0387 41.4939] [24.4739 19.2294 78.8544]|0.16s & 0.0 s\n",
      "11100 [12.5548  9.9093 48.18  ] [24.3075 18.8807 82.5216]|0.17s & 0.0 s\n",
      "11200 [12.3053  9.8478 41.5692] [24.5809 19.3407 78.1138]|0.2s & 0.0 s\n",
      "11300 [12.439   9.8524 42.8586] [24.0312 18.7531 80.6214]|0.18s & 0.0 s\n",
      "11400 [12.2491  9.7241 41.4413] [24.0607 18.7909 81.4867]|0.16s & 0.0 s\n",
      "11500 [12.6374 10.1353 40.9209] [25.2416 19.8901 78.2566]|*||0.17s & 0.0 s\n",
      "11600 [12.2591  9.6729 46.3456] [24.0359 18.7237 80.4221]|0.2s & 0.0 s\n",
      "11700 [15.7633 12.4029 54.7193] [24.4096 18.4831 89.1425]|0.18s & 0.0 s\n",
      "11800 [13.3898 10.4348 50.0935] [23.7365 18.1189 86.5923]|0.2s & 0.0 s\n",
      "11900 [12.5803  9.8634 46.165 ] [23.7163 18.2413 82.4438]|0.19s & 0.0 s\n",
      "12000 [12.5459 10.0399 38.4051] [25.0663 19.8033 74.4202]|*||0.18s & 0.0 s\n",
      "12100 [12.2824  9.5884 48.0614] [23.5645 18.1161 80.4973]|0.2s & 0.0 s\n",
      "12200 [15.853  12.4794 55.1778] [24.4605 18.6044 87.7023]|*||0.18s & 0.0 s\n",
      "12300 [12.2495  9.5874 47.14  ] [23.5411 18.062  81.8616]|0.19s & 0.0 s\n",
      "12400 [12.2511  9.5953 46.2462] [23.5518 18.0638 82.5516]|0.18s & 0.0 s\n",
      "12500 [12.3432  9.8571 38.3623] [24.9574 19.7324 74.6657]|*||0.22s & 0.0 s\n",
      "12600 [11.6319  9.3094 36.6915] [24.4819 19.2297 78.2452]|**||0.17s & 0.0 s\n",
      "12700 [12.5295  9.8243 46.5138] [23.5314 17.9987 81.6245]|0.18s & 0.0 s\n",
      "12800 [14.2835 11.2656 52.364 ] [23.8639 18.0869 85.5237]|0.17s & 0.0 s\n",
      "12900 [11.351   9.0912 36.2337] [24.0143 18.7826 76.2151]|0.16s & 0.0 s\n",
      "13000 [15.0936 11.8553 54.0652] [23.963 18.113 86.412]|0.16s & 0.0 s\n",
      "13100 [14.2057 11.5915 39.8565] [26.3701 21.3004 79.9228]|*||0.17s & 0.0 s\n",
      "13200 [11.3891  8.9538 41.1575] [23.2438 17.8562 82.9808]|0.18s & 0.0 s\n",
      "13300 [13.173  10.3558 47.7024] [23.3898 17.6712 84.7314]|0.17s & 0.0 s\n",
      "13400 [11.1804  8.8229 37.5761] [23.0807 17.8399 78.8342]|0.16s & 0.0 s\n",
      "13500 [12.8879 10.1152 48.0611] [23.2797 17.7332 83.7243]|0.16s & 0.0 s\n",
      "13600 [11.4748  8.9794 43.3525] [23.0477 17.684  80.435 ]|0.18s & 0.0 s\n",
      "13700 [11.8115  9.2882 43.6731] [22.8983 17.4303 81.6811]|0.17s & 0.0 s\n",
      "13800 [10.9057  8.728  35.6453] [23.582  18.4025 76.4956]|0.17s & 0.0 s\n",
      "13900 [10.7705  8.6041 35.0835] [23.3309 18.0915 75.9892]|0.16s & 0.0 s\n",
      "14000 [13.5675 10.6365 46.9657] [23.3664 17.7301 83.9844]|0.15s & 0.0 s\n",
      "14100 [12.9362 10.1804 42.8438] [23.3545 17.7145 85.8783]|0.17s & 0.0 s\n",
      "14200 [11.8555  9.2515 39.9123] [22.9013 17.4161 79.6626]|0.19s & 0.0 s\n",
      "14300 [10.5617  8.5348 34.7956] [23.4963 18.3774 73.6288]|0.21s & 0.0 s\n",
      "14400 [10.699   8.6177 33.2285] [23.4867 18.4178 72.6879]|0.17s & 0.0 s\n",
      "14500 [14.2596 11.6088 39.1336] [26.2992 21.4341 76.0808]|*||0.16s & 0.0 s\n",
      "14600 [10.3902  8.2275 35.6276] [22.8902 17.6356 75.7757]|0.22s & 0.0 s\n",
      "14700 [10.4457  8.2888 35.3702] [23.0097 17.8024 73.4785]|0.18s & 0.0 s\n",
      "14800 [10.2619  8.2395 33.6803] [23.386  18.1602 74.4039]|0.16s & 0.0 s\n",
      "14900 [10.2995  8.2686 33.7249] [23.1711 18.0311 72.026 ]|0.17s & 0.0 s\n",
      "15000 [10.5652  8.4809 30.3991] [23.4302 18.4181 73.8236]|0.22s & 0.0 s\n",
      "15100 [11.0353  8.9263 35.0889] [23.9019 18.8791 73.1381]|*||0.17s & 0.0 s\n",
      "15200 [13.8986 11.4747 36.4108] [26.3542 21.3153 82.348 ]|**||0.16s & 0.0 s\n",
      "15300 [13.3348 10.5515 46.3443] [23.2752 17.5545 84.5327]|0.2s & 0.0 s\n",
      "15400 [10.0303  8.0002 32.4779] [23.2393 18.069  74.1298]|0.2s & 0.0 s\n",
      "15500 [16.8447 14.4151 41.0099] [28.6413 23.692  84.3454]|*||0.17s & 0.0 s\n",
      "15600 [12.3092  9.6502 43.4572] [22.7583 17.1605 81.042 ]|0.16s & 0.0 s\n",
      "15700 [10.6779  8.6229 30.6781] [23.8656 18.783  73.9903]|*||0.18s & 0.0 s\n",
      "15800 [10.2971  8.1384 34.8434] [22.5419 17.1012 78.4053]|0.2s & 0.0 s\n",
      "15900 [ 9.7902  7.8582 34.4853] [23.0894 17.9079 73.7277]|0.19s & 0.0 s\n",
      "16000 [ 9.7221  7.778  34.8573] [22.9079 17.646  73.6727]|0.2s & 0.0 s\n",
      "16100 [ 9.7253  7.788  30.0345] [22.7544 17.5047 72.2348]|0.18s & 0.0 s\n",
      "16200 [11.2179  8.8015 39.6091] [22.5691 17.0175 79.4601]|0.17s & 0.0 s\n",
      "16300 [14.9955 12.0061 49.6572] [23.6565 17.824  88.5186]|*||0.18s & 0.0 s\n",
      "16400 [ 9.6693  7.7148 35.3704] [22.7014 17.4311 74.6001]|0.16s & 0.0 s\n",
      "16500 [ 9.9329  7.8537 36.2246] [22.4045 17.0099 78.9258]|0.23s & 0.0 s\n",
      "16600 [11.4372  9.0608 41.0452] [22.6815 17.0661 84.0824]|0.19s & 0.0 s\n",
      "16700 [ 9.9143  7.9899 29.4317] [23.5224 18.3896 77.7659]|*||0.19s & 0.0 s\n",
      "16800 [ 9.3522  7.4279 30.5917] [22.4328 17.1917 74.7499]|0.21s & 0.0 s\n",
      "16900 [11.9639  9.4287 42.2391] [22.6377 16.9401 84.1864]|0.19s & 0.0 s\n",
      "17000 [10.5773  8.3266 37.9992] [22.4278 16.871  81.8812]|0.2s & 0.0 s\n",
      "17100 [12.8746 10.6417 34.6487] [25.4215 20.4926 86.9359]|*||0.2s & 0.0 s\n",
      "17200 [10.0193  7.8569 35.7534] [22.3444 16.8235 78.2663]|0.19s & 0.0 s\n",
      "17300 [10.363   8.3575 30.6267] [23.7949 18.7874 76.0677]|*||0.2s & 0.0 s\n",
      "17400 [ 9.1861  7.3027 29.7645] [22.5902 17.3479 73.447 ]|0.19s & 0.0 s\n",
      "17500 [10.1268  8.1429 29.7663] [23.7527 18.6577 83.3919]|*||0.2s & 0.0 s\n",
      "17600 [10.1084  8.123  31.2891] [23.708  18.5674 79.5726]|**||0.18s & 0.0 s\n",
      "17700 [ 9.8512  7.7208 33.6953] [22.2841 16.7482 79.8464]|0.18s & 0.0 s\n",
      "17800 [ 9.1984  7.3373 31.1199] [22.4866 17.1208 73.6088]|0.16s & 0.0 s\n",
      "17900 [ 9.6318  7.6178 32.861 ] [22.224  16.72   78.9303]|0.2s & 0.0 s\n",
      "18000 [14.8509 11.999  43.4146] [23.7215 17.8551 85.907 ]|*||0.18s & 0.0 s\n",
      "18100 [ 9.1085  7.3405 28.9767] [22.8039 17.5692 73.8687]|0.17s & 0.0 s\n",
      "18200 [ 8.9234  7.1421 27.4911] [22.5868 17.4288 73.2169]|0.16s & 0.0 s\n",
      "18300 [ 8.8674  7.1285 28.7977] [22.6121 17.372  73.8022]|0.17s & 0.0 s\n",
      "18400 [ 8.8238  7.1315 26.035 ] [22.3205 17.0097 75.1522]|0.2s & 0.0 s\n",
      "18500 [ 9.4524  7.6468 30.9795] [23.2994 18.0738 80.7499]|*||0.19s & 0.0 s\n",
      "18600 [12.5296 10.0188 40.558 ] [22.6896 16.9535 89.5374]|0.17s & 0.0 s\n",
      "18700 [10.1662  8.1852 29.1411] [23.8134 18.8188 78.2226]|*||0.17s & 0.0 s\n",
      "18800 [ 9.2593  7.3336 32.1424] [21.9791 16.659  76.8031]|0.2s & 0.0 s\n",
      "18900 [ 8.7297  6.9351 29.3369] [22.1308 16.8652 74.176 ]|0.19s & 0.0 s\n",
      "19000 [ 8.6493  6.9817 26.757 ] [22.284  17.0398 71.8571]|0.21s & 0.0 s\n",
      "19100 [ 9.08    7.1442 29.9214] [21.9612 16.6026 76.6799]|0.22s & 0.0 s\n",
      "19200 [13.3664 10.7314 39.8268] [22.9694 17.3345 85.5909]|*||0.2s & 0.01 s\n",
      "19300 [ 8.6449  6.9094 28.0378] [22.2257 17.0466 75.9481]|0.2s & 0.0 s\n",
      "19400 [ 8.6631  6.9775 27.9723] [22.4813 17.2549 73.7202]|0.18s & 0.0 s\n",
      "19500 [ 8.9773  7.1383 28.2203] [21.8667 16.5557 78.1467]|0.17s & 0.0 s\n",
      "19600 [11.2677  8.9046 35.1746] [22.3431 16.7946 86.3537]|0.18s & 0.0 s\n",
      "19700 [ 9.4496  7.5282 32.0199] [21.8061 16.3953 79.8971]|0.19s & 0.0 s\n",
      "19800 [ 8.3857  6.7265 26.2374] [22.1436 16.921  76.5318]|0.19s & 0.0 s\n",
      "19900 [ 9.1835  7.2432 29.5504] [21.7228 16.3857 80.6399]|0.18s & 0.0 s\n",
      "20000 [ 8.6722  6.92   27.5701] [22.6582 17.4964 75.0439]|*||0.19s & 0.0 s\n",
      "20100 [ 8.3452  6.6721 27.0013] [22.2351 17.1221 73.258 ]|0.17s & 0.0 s\n",
      "20200 [16.7763 14.1897 42.9828] [24.5205 18.5037 91.0519]|*||0.17s & 0.0 s\n",
      "20300 [ 8.7844  7.045  24.8334] [22.506  17.5011 77.6762]|**||0.16s & 0.0 s\n",
      "20400 [ 8.3203  6.6453 26.511 ] [21.7481 16.4853 78.3707]|0.2s & 0.0 s\n",
      "20500 [ 8.2663  6.5917 25.5384] [22.0328 16.9488 73.3496]|0.2s & 0.0 s\n",
      "20600 [ 9.0505  7.2432 29.2061] [22.7931 17.7724 78.1023]|*||0.19s & 0.0 s\n",
      "20700 [11.0676  8.7511 34.5021] [21.9734 16.5077 80.3319]|0.18s & 0.0 s\n",
      "20800 [ 8.1934  6.5329 26.0545] [21.7263 16.4987 74.9588]|0.21s & 0.0 s\n",
      "20900 [ 8.0636  6.4243 25.8731] [21.6689 16.457  75.046 ]|0.18s & 0.0 s\n",
      "21000 [10.9006  8.6591 34.0438] [21.9356 16.5162 82.1812]|0.17s & 0.0 s\n",
      "21100 [ 8.0941  6.4118 24.2597] [21.5498 16.3542 73.4518]|0.21s & 0.0 s\n",
      "21200 [ 7.8622  6.2672 23.4048] [21.82   16.7097 75.4478]|0.17s & 0.0 s\n",
      "21300 [ 8.7159  6.9846 26.5511] [22.5696 17.6026 79.4918]|*||0.17s & 0.0 s\n",
      "21400 [ 8.8195  6.9328 29.4863] [21.609  16.3134 79.8627]|0.2s & 0.0 s\n",
      "21500 [ 8.8     6.9666 27.8571] [21.487  16.2583 79.2166]|0.17s & 0.0 s\n",
      "21600 [ 9.9788  7.9462 28.7418] [21.6246 16.273  80.0224]|0.17s & 0.0 s\n",
      "21700 [ 8.9625  7.2584 27.0983] [22.7051 17.7174 81.6338]|*||0.16s & 0.0 s\n",
      "21800 [ 7.9466  6.3827 22.3424] [22.0068 16.909  75.445 ]|0.22s & 0.0 s\n",
      "21900 [11.8128  9.5328 31.8427] [22.2577 16.7204 85.3826]|*||0.18s & 0.0 s\n",
      "22000 [ 8.4892  6.7318 25.7139] [21.5404 16.3033 78.9463]|0.19s & 0.0 s\n",
      "22100 [10.0561  8.0095 29.1953] [21.631  16.3288 80.7363]|0.16s & 0.0 s\n",
      "22200 [ 7.7284  6.2544 22.2969] [21.8239 16.7511 73.4634]|0.17s & 0.0 s\n",
      "22300 [ 8.7556  6.8877 29.3879] [21.5208 16.3473 77.2216]|0.16s & 0.0 s\n",
      "22400 [ 8.4251  6.6904 27.366 ] [21.4643 16.2617 76.5716]|0.17s & 0.0 s\n",
      "22500 [ 9.6087  7.8075 26.9906] [23.1755 18.1783 87.2237]|*||0.18s & 0.0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22600 [ 9.0438  7.363  23.351 ] [22.8319 17.8948 84.7548]|**||0.17s & 0.0 s\n",
      "22700 [ 8.0119  6.3167 25.9526] [21.5147 16.3373 77.0347]|0.16s & 0.0 s\n",
      "22800 [11.4939  9.2644 34.9758] [21.9357 16.5887 82.2617]|0.16s & 0.0 s\n",
      "22900 [ 9.1163  7.4085 24.2629] [23.0038 18.0318 77.0943]|*||0.16s & 0.0 s\n",
      "23000 [ 7.6106  6.0372 23.401 ] [21.6405 16.5858 74.1882]|0.16s & 0.0 s\n",
      "23100 [ 7.9891  6.4321 22.2814] [22.0764 17.1996 78.7604]|0.16s & 0.0 s\n",
      "23200 [14.6127 12.8124 35.6635] [26.5028 21.5048 95.201 ]|*||0.16s & 0.0 s\n",
      "23300 [ 7.3968  5.941  23.4451] [21.3899 16.3517 74.1134]|0.16s & 0.0 s\n",
      "23400 [12.6899 10.8736 30.4784] [25.1119 20.2465 93.4801]|*||0.17s & 0.0 s\n",
      "23500 [ 7.7716  6.1718 26.5869] [21.2512 16.1431 75.9297]|0.16s & 0.0 s\n",
      "23600 [ 7.8627  6.3567 24.7586] [21.9902 17.0205 75.332 ]|*||0.15s & 0.0 s\n",
      "23700 [ 7.4255  5.9172 23.3272] [21.2486 16.219  75.2076]|0.17s & 0.0 s\n",
      "23800 [14.4423 12.2116 36.9596] [23.1746 17.565  89.8087]|*||0.17s & 0.0 s\n",
      "23900 [ 9.4325  7.5729 27.6906] [21.3589 16.1065 82.7005]|0.16s & 0.0 s\n",
      "24000 [ 7.474   5.9739 23.9823] [21.3034 16.2496 72.592 ]|0.16s & 0.0 s\n",
      "24100 [ 8.399   6.7956 24.7968] [22.1641 17.313  77.4486]|*||0.16s & 0.0 s\n",
      "24200 [14.9259 12.56   39.6509] [23.4269 17.7939 89.2909]|**||0.17s & 0.0 s\n",
      "24300 [ 7.6292  6.0706 21.1416] [21.0845 16.1284 71.3439]|0.16s & 0.0 s\n",
      "24400 [ 7.213   5.7787 19.1192] [21.3704 16.4315 73.1672]|0.16s & 0.0 s\n",
      "24500 [ 7.3491  5.9129 19.3587] [21.315  16.4183 72.1722]|0.15s & 0.0 s\n",
      "24600 [10.5556  8.5138 30.4063] [21.4406 16.2109 82.9713]|0.16s & 0.0 s\n",
      "24700 [ 8.3409  6.6373 23.5516] [21.0015 15.9226 79.1773]|0.21s & 0.0 s\n",
      "24800 [ 7.9569  6.4154 21.964 ] [21.8906 17.1428 76.9967]|*||0.16s & 0.0 s\n",
      "24900 [ 7.3761  5.9279 19.815 ] [21.5472 16.7032 74.0984]|0.16s & 0.0 s\n",
      "25000 [ 7.1771  5.7656 20.642 ] [21.2044 16.3167 73.698 ]|0.16s & 0.0 s\n",
      "25100 [10.3201  8.3114 28.2268] [21.3948 16.1703 80.9879]|0.2s & 0.0 s\n",
      "25200 [ 7.7478  6.1725 24.1596] [21.5842 16.7815 77.7571]|0.17s & 0.0 s\n",
      "25300 [ 7.0152  5.5771 20.6795] [20.9748 16.0334 70.754 ]|0.17s & 0.0 s\n",
      "25400 [10.4816  8.5122 29.4047] [21.3273 16.0572 83.4763]|0.15s & 0.0 s\n",
      "25500 [ 8.9982  7.1679 27.9081] [21.0841 15.982  81.7597]|0.16s & 0.0 s\n",
      "25600 [10.2626  8.2865 29.3762] [21.376  16.1815 82.0318]|0.18s & 0.0 s\n",
      "25700 [ 7.8444  6.1924 24.1466] [20.7413 15.6874 76.4323]|0.17s & 0.0 s\n",
      "25800 [ 8.0475  6.3533 24.818 ] [20.9365 15.8436 77.0126]|0.16s & 0.0 s\n",
      "25900 [ 9.9289  7.9544 31.4526] [21.3052 16.1578 80.9104]|0.16s & 0.0 s\n",
      "26000 [ 8.218   6.522  24.8284] [20.8644 15.8431 78.235 ]|0.17s & 0.0 s\n",
      "26100 [ 8.2946  6.7512 21.1589] [22.1582 17.4446 78.2346]|*||0.18s & 0.0 s\n",
      "26200 [ 6.8318  5.4938 21.0844] [20.9527 16.0416 69.9383]|0.21s & 0.0 s\n",
      "26300 [ 7.5643  5.8956 23.7167] [20.9326 15.9084 72.6746]|0.17s & 0.0 s\n",
      "26400 [ 6.848   5.4995 18.3778] [20.9674 15.9913 72.9621]|0.17s & 0.0 s\n",
      "26500 [ 9.774   7.8457 26.8417] [21.1537 16.0462 78.9599]|0.2s & 0.01 s\n",
      "26600 [ 8.4227  6.7248 23.9409] [20.7788 15.7174 75.6329]|0.18s & 0.0 s\n",
      "26700 [ 9.6506  7.7184 26.7094] [21.1878 16.1021 77.2251]|0.19s & 0.0 s\n",
      "26800 [ 7.9265  6.284  23.5227] [20.7013 15.7107 73.1454]|0.16s & 0.0 s\n",
      "26900 [11.3204  9.3326 30.5546] [21.5804 16.3538 81.8574]|*||0.19s & 0.0 s\n",
      "27000 [12.9515 10.8661 31.8751] [22.1782 16.7416 86.2393]|**||0.17s & 0.0 s\n",
      "27100 [ 7.7693  6.1822 23.8637] [20.7898 15.8604 74.5699]|0.18s & 0.0 s\n",
      "27200 [ 6.8246  5.4353 20.6597] [20.7502 15.8476 70.9517]|0.17s & 0.0 s\n",
      "27300 [ 8.2288  6.7145 20.8263] [22.0839 17.3253 73.101 ]|*||0.19s & 0.0 s\n",
      "27400 [ 7.5781  6.0436 23.9112] [20.8483 15.8488 76.4469]|0.18s & 0.0 s\n",
      "27500 [ 6.8279  5.4848 18.7506] [21.045  16.3513 68.63  ]|0.17s & 0.0 s\n",
      "27600 [ 8.7677  6.985  24.4456] [20.7271 15.7004 76.8568]|0.17s & 0.0 s\n",
      "27700 [ 7.7335  6.2247 21.5171] [21.7969 17.0869 80.4685]|*||0.17s & 0.0 s\n",
      "27800 [ 7.0296  5.5215 21.4546] [20.5651 15.7497 69.6283]|0.19s & 0.0 s\n",
      "27900 [ 6.8867  5.5659 19.0817] [21.1208 16.3825 69.9189]|0.17s & 0.0 s\n",
      "28000 [10.4663  8.8086 30.6758] [23.1395 18.606  78.5692]|*||0.15s & 0.0 s\n",
      "28100 [ 6.687  5.329 19.736] [20.7105 15.8729 70.1792]|0.16s & 0.0 s\n",
      "28200 [ 7.0546  5.6279 20.5675] [20.5625 15.6943 69.5079]|0.19s & 0.0 s\n",
      "28300 [ 7.5726  6.0253 22.9447] [20.4785 15.533  73.2936]|0.19s & 0.0 s\n",
      "28400 [ 8.3362  6.631  25.0742] [20.7099 15.7378 73.8139]|0.18s & 0.0 s\n",
      "28500 [ 7.7755  6.3022 21.2355] [21.6251 17.02   67.877 ]|*||0.18s & 0.0 s\n",
      "28600 [ 8.132   6.6002 22.0214] [22.0586 17.3964 72.8796]|**||0.19s & 0.0 s\n",
      "28700 [ 9.4717  7.9183 25.9845] [22.7086 18.1417 77.7823]|***||$$$\n",
      "learning rate: 8.0000E-04, global step: 28700\n"
     ]
    }
   ],
   "source": [
    "lr, gs = sess.run([learning_rate, global_step])\n",
    "print('learning rate: {:.4E}, global step: {}'.format(lr, gs))\n",
    "prev_test_error = 1e+10\n",
    "t = time.time()\n",
    "for cnt in range(init_epochs):\n",
    "    epoch += 1\n",
    "    # training itself\n",
    "    \n",
    "    for batch in iterate_minibatches(X_train_current, y_train_current, batch_size):\n",
    "        X_batch, y_batch = batch\n",
    "        sess.run(train_step, feed_dict={x: X_batch, \n",
    "                                        y_: y_batch, \n",
    "                                        keep_probability_: keep_prob, \n",
    "                                        l2_reg_: l2_reg})\n",
    "    # checking errors\n",
    "    if (cnt+1) % early_stopping_check_step == 0:\n",
    "        print(np.round(time.time() - t, 2), end='s')\n",
    "        t = time.time()\n",
    "        preds_train = sess.run(y, feed_dict={x: X_train_current, keep_probability_: 1})\n",
    "        preds_test = sess.run(y, feed_dict= {x: X_test , keep_probability_: 1})\n",
    "        \n",
    "        train_err =  get_errors(preds_train, y_train_current)\n",
    "        test_err =  get_errors(preds_test, y_test)\n",
    "        print(' &', np.round(time.time() - t, 2), 's')\n",
    "        print(epoch, np.round(train_err, 4), np.round(test_err, 4), end = '|')\n",
    "        data.append([epoch] + train_err + test_err)\n",
    "        # checking early stopping conditions\n",
    "        if (test_err[0] > prev_test_error*(1 + early_stopping_window)) and (cnt > mandatory_uptrain_epochs):\n",
    "            warnings += 1\n",
    "            print('*'*warnings, end = '||')\n",
    "            if warnings >= max_warnings:\n",
    "                print('$$$')\n",
    "                break\n",
    "        else:\n",
    "            warnings = 0\n",
    "            prev_test_error = min(test_err[0], prev_test_error)\n",
    "        t = time.time()\n",
    "lr, gs = sess.run([learning_rate, global_step])\n",
    "print('learning rate: {:.4E}, global step: {}'.format(lr, gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model saved in path: /Users/romaushakov/Desktop/diploma/init_rosenbrock_exp.ckpt\n"
     ]
    }
   ],
   "source": [
    "fname_identifier = \"rosenbrock_exp\"\n",
    "save_path = saver.save(sess, \"/Users/romaushakov/Desktop/diploma/init_\" + fname_identifier + \".ckpt\")\n",
    "print(\"Init model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/romaushakov/Desktop/diploma/init_rosenbrock_exp.ckpt\n",
      "Init model restored\n"
     ]
    }
   ],
   "source": [
    "fname_identifier = \"rosenbrock_exp\"\n",
    "saver.restore(sess, \"/Users/romaushakov/Desktop/diploma/init_\" + fname_identifier + \".ckpt\")\n",
    "print(\"Init model restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_current = X_train.copy()\n",
    "y_train_current = y_train.copy()\n",
    "X_pool_current = X_pool.copy()\n",
    "y_pool_current = y_pool.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_block_inv(A_inv, B, C, D):\n",
    "    H = D - C.dot(A_inv).dot(B)\n",
    "    H_inv = 1./ H\n",
    "    a00 = A_inv + H_inv * A_inv.dot(B).dot(C).dot(A_inv)\n",
    "    a01 = -A_inv.dot(B) * H_inv\n",
    "    a10 = -H_inv * C.dot(A_inv)\n",
    "    a11 = H_inv\n",
    "    \n",
    "    return np.block([[a00, a01.reshape(-1, 1)],\n",
    "                    [a10.reshape((1, -1)), np.array(a11).reshape((1, 1))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Integral-based ALGO\n",
      "========================================\n",
      "Starting iteration # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "gpnn_max_train = 1000\n",
    "points_to_integrate = 1000\n",
    "\n",
    "print('='*40)\n",
    "print('Integral-based ALGO')\n",
    "print('='*40)\n",
    "\n",
    "for al_iters in range(al_steps):\n",
    "    # 1) get MCDUEs\n",
    "    t = time.time()\n",
    "    print('Starting iteration #', al_iters)\n",
    "    random_train_inds = np.random.permutation(range(len(X_train_current)))[:gpnn_max_train]\n",
    "    random_train_samples = X_train_current[random_train_inds]\n",
    "    \n",
    "    train_and_pool_samples = np.concatenate([random_train_samples, X_pool_current])    \n",
    "    stds = get_stds(train_and_pool_samples)\n",
    "    \n",
    "    K_train_cov = np.cov(stds[:gpnn_max_train, :], ddof = 0)\n",
    "    K_train_cov_inv = np.linalg.inv(K_train_cov + diag_eps * np.eye(gpnn_max_train))\n",
    "    \n",
    "    minimums = random_train_samples.min(axis=0)\n",
    "    maximums = random_train_samples.max(axis=0)\n",
    "    \n",
    "    ### vs are points for integral\n",
    "    vs = np.random.uniform(minimums, maximums,\n",
    "                           size=(points_to_integrate, \n",
    "                                 random_train_samples.shape[1]))\n",
    "    \n",
    "    # get mcdues for random vs\n",
    "    y_vs = get_stds(vs)\n",
    "\n",
    "    ### sigma(v | X) for each v in vs\n",
    "    sigmas = []\n",
    "    for cnt_ in range(len(vs)):\n",
    "        vs_sample = y_vs[cnt_, :]\n",
    "        Q = simple_cov(stds[:gpnn_max_train], vs_sample)[:, None]\n",
    "        KK = np.var(vs_sample)\n",
    "        sigma = KK - np.dot(np.dot(Q.T, K_train_cov_inv), Q)[0][0]\n",
    "        sigmas.append(np.sqrt(sigma))\n",
    "    \n",
    "\n",
    "    # for each x in X_pool_current:\n",
    "    # we count \\int sigma(v|X) - sigma(v|X+x_from_pool) dv\n",
    "    diffs_integral = np.zeros(X_pool_current.shape[0])\n",
    "    \n",
    "    ### extend cov matrix \n",
    "    new_K_cov = np.zeros((gpnn_max_train + 1, gpnn_max_train + 1))\n",
    "    new_K_cov[:gpnn_max_train, :gpnn_max_train] = K_train_cov\n",
    "    \n",
    "    ### loop over pool data\n",
    "    for x_cnt_ in tqdm.tqdm(range(len(X_pool_current))):\n",
    "        \n",
    "        # stds was recieved for train_and_pool_samples \n",
    "        # and train_pool_sample = np.concatenate([random_train_sample, X_pool_current])\n",
    "        # and random_train_samples.shape[0] = gpnn_max_train. So\n",
    "        \n",
    "        \n",
    "        # extend cov matrix\n",
    "        # we don't recalculate all cov matrix\n",
    "        # we only add one row \n",
    "        pool_sample = stds[(gpnn_max_train + x_cnt_), :]\n",
    "        Q = simple_cov(stds[:gpnn_max_train, :], pool_sample)[:, None]\n",
    "        Q = Q.ravel()\n",
    "        new_K_cov[-1, :-1] = Q\n",
    "        new_K_cov[:-1, -1] = Q\n",
    "        new_K_cov[-1, -1] = np.var(pool_sample)\n",
    "        new_K_cov_inv = compute_block_inv(K_train_cov_inv + np.eye(K_train_cov_inv.shape[0]) * diag_eps,\n",
    "                                          Q.reshape((-1, 1)),\n",
    "                                          Q.reshape((1, -1)), \n",
    "                                          np.var(pool_sample) + diag_eps)\n",
    "        \n",
    "        \n",
    "        indices = list(range(gpnn_max_train)) + [gpnn_max_train + x_cnt_]\n",
    "\n",
    "        ### count sigma(v | X + x_from_pool) with extended \n",
    "        ### cov matrix for each v in vs\n",
    "        extended_sigmas = []\n",
    "        for cnt_ in range(len(y_vs)):\n",
    "            vs_sample = y_vs[cnt_, :]\n",
    "            Q = simple_cov(stds[indices], vs_sample)[:, None]\n",
    "            KK = np.var(vs_sample)\n",
    "            sigma = KK - np.dot(np.dot(Q.T, new_K_cov_inv), Q)[0][0]\n",
    "            extended_sigmas.append(sigma)\n",
    "        \n",
    "        break\n",
    "    break\n",
    "#         current_diff = np.array(sigmas) - np.array(extended_sigmas)\n",
    "#         diffs_integral[x_cnt_] = current_diff.sum()\n",
    "\n",
    "        \n",
    "#     inds = np.argsort(diffs_integral)[::-1][:sample_each_step]\n",
    "#     X_train_current = np.concatenate([X_train_current, X_pool_current[inds, :]])\n",
    "#     y_train_current = np.concatenate([y_train_current, y_pool_current[inds, :]])\n",
    "#     print('Added to training set, new sizes:', X_train_current.shape, y_train_current.shape)\n",
    "#     # 4) remove them from the pool\n",
    "#     X_pool_current = np.delete(X_pool_current, inds, axis = 0)\n",
    "#     y_pool_current = np.delete(y_pool_current, inds, axis = 0)\n",
    "#     print('Deleted from pool set, new sizes:', X_pool_current.shape, y_pool_current.shape)\n",
    "#     # 5) uptrain the NN\n",
    "#     prev_test_error = 1e+10\n",
    "#     sample_selection_time = time.time() - t\n",
    "#     t_big = time.time()\n",
    "#     t = time.time()\n",
    "#     for cnt in range(uptrain_epochs):\n",
    "#         epoch += 1\n",
    "#         # training itself\n",
    "\n",
    "#         for batch in iterate_minibatches(X_train_current, y_train_current, batch_size):\n",
    "#             X_batch, y_batch = batch\n",
    "#             sess.run(train_step, feed_dict={x: X_batch, \n",
    "#                                             y_: y_batch, \n",
    "#                                             keep_probability_: keep_prob, \n",
    "#                                             l2_reg_: l2_reg})\n",
    "#         # checking errors\n",
    "#         if (cnt+1) % early_stopping_check_step == 0:\n",
    "#             print(np.round(time.time() - t, 2), end='s')\n",
    "#             t = time.time()\n",
    "#             preds_train = sess.run(y, feed_dict={x: X_train_current, keep_probability_: 1})\n",
    "#             preds_test = sess.run(y, feed_dict= {x: X_test , keep_probability_: 1})\n",
    "\n",
    "#             train_err =  get_errors(preds_train, y_train_current)\n",
    "#             test_err =  get_errors(preds_test, y_test)\n",
    "#             print(' &', np.round(time.time() - t, 2), 's')\n",
    "#             print(epoch, np.round(train_err, 4), np.round(test_err, 4), end = '|')\n",
    "#             # data.append([al_iters] + train_err + test_err)\n",
    "#             # checking early stopping conditions\n",
    "#             if (test_err[0] > prev_test_error*(1 + early_stopping_window)) and (cnt > mandatory_uptrain_epochs):\n",
    "#                 warnings += 1\n",
    "#                 print('*'*warnings, end = '||')\n",
    "#                 if warnings >= max_warnings:\n",
    "#                     print('$$$')\n",
    "#                     break\n",
    "#             else:\n",
    "#                 warnings = 0\n",
    "#                 prev_test_error = min(test_err[0], prev_test_error)\n",
    "#                 save_path = saver.save(sess, \"/Users/romaushakov/Desktop/diploma/init_\" + fname_identifier + \".ckpt\")\n",
    "#                 print(\"MCDUE model saved in path: %s\" % save_path)\n",
    "#             t = time.time()\n",
    "            \n",
    "#     print('NN uptrained')\n",
    "#     uptraining_time = time.time() - t_big\n",
    "#     saver.restore(sess, \"/Users/romaushakov/Desktop/diploma/init_\" + fname_identifier + \".ckpt\")\n",
    "#     preds_train = sess.run(y, feed_dict={x: X_train_current, keep_probability_: 1})\n",
    "#     preds_test = sess.run(y, feed_dict= {x: X_test , keep_probability_: 1})\n",
    "\n",
    "#     train_err =  get_errors(preds_train, y_train_current)\n",
    "#     test_err =  get_errors(preds_test, y_test)\n",
    "#     print(epoch, np.round(train_err, 4), np.round(test_err, 4), end = '|')\n",
    "#     data.append([al_iters, sample_selection_time, uptraining_time] + train_err + test_err)\n",
    "#     lr, gs = sess.run([learning_rate, global_step])\n",
    "#     print('learning rate: {:.4E}, global step: {}'.format(lr, gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_sigmas = np.array(extended_sigmas)\n",
    "sum(extended_sigmas < 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas = np.array(sigmas)\n",
    "sum(sigmas < 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
